# üèóÔ∏è Architecture TRM (Mistral 7B ‚Üî BERT) - PRD Enrichi

**Version :** 2.1 (POC Bootstrap)  
**Date :** D√©cembre 2025  
**Concept :** Architecture collaborative permettant √† un mod√®le g√©n√©ratif (Mistral 7B) de traiter efficacement un contexte court (~500 tokens) en s'appuyant sur une m√©moire condens√©e et pond√©r√©e fournie par un mod√®le encodeur (BERT), avec int√©gration RAG par philosophe.

**üí∞ Mode Bootstrap 0‚Ç¨ ‚Üí 100‚Ç¨ :** Ce document d√©crit une roadmap optimis√©e pour un budget minimal (100‚Ç¨ max), avec d√©veloppement sur Colab gratuit et infrastructure Vast.ai/RunPod. Le POC est con√ßu comme un **asset valorisable** (consulting, formation, portfolio) autant qu'une validation technique.

**üéØ Objectif Strat√©gique :** R√©duire les co√ªts d'inf√©rence LLM de 50% tout en cr√©ant une expertise diff√©renciante valorisable (TRM = tendance 2026).

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Architecture Technique](#architecture-technique)
3. [Composants Principaux](#composants-principaux)
4. [Pipeline Fonctionnel](#pipeline-fonctionnel)
5. [Mod√®les de Donn√©es](#mod√®les-de-donn√©es)
6. [Roadmap d'Impl√©mentation](#roadmap-dimpl√©mentation)
7. [M√©triques & Tests](#m√©triques--tests)
8. [Exemples Concrets](#exemples-concrets)

---

## üéØ Vue d'Ensemble

### Objectif Principal

Permettre √† un mod√®le g√©n√©ratif (Mistral 7B) de traiter efficacement un contexte court (~500 tokens) en s'appuyant sur une m√©moire condens√©e et pond√©r√©e fournie par un mod√®le encodeur (BERT), tout en conservant un flux d'information structur√© et inductif, avec acc√®s √† une m√©moire externe via RAG par philosophe.

### Objectifs Business & Strat√©giques

**üí∞ Optimisation Co√ªts** :
- R√©duction co√ªts inf√©rence : **-50%** (Qwen 14B ‚Üí TRM Mistral 7B)
- Scaling horizontal : GPU 7B moins cher que 14B
- M√™me budget ‚Üí 2-3√ó plus de requ√™tes trait√©es

**üöÄ Positionnement Early Adopter** :
- TRM = tendance 2026 (Small LM + Encoders)
- POC = case study avant que √ßa devienne mainstream
- Expertise diff√©renciante valorisable (consulting, formation, portfolio)

**üíº Valorisation Expertise** :
- White paper / blog post (Medium, Dev.to)
- Talk / conf√©rence (PyConFR, MLOps meetups)
- Portfolio GitHub (cr√©dibilit√© technique)
- Consulting / freelance (5-10k‚Ç¨ par mission)

### Principes Cl√©s

- **M√©moire conversationnelle condens√©e** : BERT encode et condense l'historique en STATE_IMAGE structur√©
- **M√©moire externe intelligente** : RAG retrieve passages pertinents du corpus philosophique, condens√©s par BERT
- **G√©n√©ration locale focalis√©e** : Mistral g√©n√®re avec contexte court, maximisant l'attention locale
- **√âvolutivit√©** : Nouveaux axes d'information et heuristiques peuvent √™tre int√©gr√©s facilement
- **Bootstrap-friendly** : D√©veloppement Colab gratuit + infrastructure payante minimale (100‚Ç¨ max)

---

## üèóÔ∏è Architecture Technique

### Stack Technologique

```
Frontend (HTML/JS)
    ‚Üì
Backend API (FastAPI)
    ‚Üì
Router (d√©termine philosophe/corpus)
    ‚Üì
RAG Retriever (sentence-transformers + FAISS)
    ‚Üì
BERT Encoder (CPU - Vast.ai/RunPod ou Colab gratuit)
    ‚îú‚îÄ‚îÄ Encode conversation + passages RAG
    ‚îú‚îÄ‚îÄ Analyse axes: concepts, intention, tension, style, priorit√©
    ‚îî‚îÄ‚îÄ G√©n√®re STATE_IMAGE condens√©
    ‚Üì
Mistral 7B (GPU - Vast.ai/RunPod RTX 4090)
    ‚îú‚îÄ‚îÄ Lit STATE_IMAGE structur√©
    ‚îú‚îÄ‚îÄ G√©n√®re r√©ponse (‚â§500 tokens contexte)
    ‚îî‚îÄ‚îÄ D√©tecte motifs/tics de langage
    ‚Üì
Mini-store (Heuristique - simplifi√© POC)
    ‚îú‚îÄ‚îÄ Collecte motifs r√©currents
    ‚îî‚îÄ‚îÄ Feedback pour mise √† jour STATE par BERT (d√©sactiv√© POC)
```

**Infrastructure Bootstrap** :
- **D√©veloppement** : Google Colab gratuit (CPU pour BERT, T4 GPU pour tests Mistral)
- **Production POC** : Vast.ai (~$0.30-0.40/h) ou RunPod (~$0.49/h)
- **Budget total** : 100‚Ç¨ max (250h Vast.ai ou 200h RunPod)

### Flux d'Ex√©cution D√©taill√©

```
1. User Input re√ßu
   ‚Üì
2. Router d√©termine philosophe (Spinoza/Kant/Bergson)
   ‚Üì
3. RAG Retrieve (parall√®le ou s√©quentiel)
   - Query: dernier √©change utilisateur + concepts actifs STATE
   - Corpus: corpus du philosophe d√©termin√©
   - Retourne: 2-3 passages pertinents max
   ‚Üì
4. BERT Encode
   - Entr√©es:
     * Dernier √©change utilisateur
     * R√©ponse pr√©c√©dente (si existe)
     * Passages RAG r√©cup√©r√©s
     * STATE pr√©c√©dent
     * Feedback Mini-store (motifs r√©currents)
   - Traitement:
     * Analyse axes: concepts, intention, tension, fr√©quence, style, priorit√©, relations, √©motions
     * Extrait concepts cl√©s des passages RAG (pas texte brut)
     * Pond√®re et condense en STATE_IMAGE
   ‚Üì
5. STATE_IMAGE g√©n√©r√© (structur√© JSON)
   - Contient concepts conversationnels + concepts RAG condens√©s
   - Pas de passages bruts (√©vite surcharge tokens)
   ‚Üì
6. Mistral 7B G√©n√®re
   - Contexte: STATE_IMAGE structur√© (~100-200 tokens)
   - Dernier √©change utilisateur (~50-100 tokens)
   - Total ‚â§ 500 tokens
   - G√©n√®re r√©ponse adapt√©e au style/philosophe
   ‚Üì
7. Mini-store mis √† jour
   - Collecte motifs r√©currents d√©tect√©s par Mistral
   - Alertes sur tics de langage
   ‚Üì
8. Feedback loop
   - Mini-store informe BERT pour prochaine mise √† jour STATE
```

---

## üß© Composants Principaux

### 1. Router (D√©termination Corpus)

**Responsabilit√©s :**
- D√©termine le philosophe/corpus √† utiliser selon contexte
- Peut √™tre bas√© sur s√©lection utilisateur ou d√©tection automatique

**Localisation :** `Backend/router.py`

**Fonctions cl√©s :**
- `determine_philosopher(user_input, session_context)` ‚Üí philosopher_id
- `get_corpus_path(philosopher_id)` ‚Üí corpus_path

**Exemple :**
```python
def determine_philosopher(user_input, session_context):
    # Si s√©lection explicite dans session
    if session_context.get('philosopher'):
        return session_context['philosopher']
    
    # Sinon d√©tection automatique (optionnel)
    # Analyse keywords/concepts dans user_input
    return "spinoza"  # default
```

### 2. RAG Retriever

**Mod√®le :** `sentence-transformers` (all-MiniLM-L6-v2 ou e5-small) + Whoosh/FAISS

**Responsabilit√©s :**
- Retrieve passages pertinents du corpus philosophique
- Filtre par philosophe (via Router)
- Limite √† 2-3 passages max pour √©viter surcharge

**Localisation :** `Backend/rag_retriever.py`

**Fonctions cl√©s :**
- `retrieve_passages(query, philosopher, top_k=3)` ‚Üí List[passage]
- `extract_concepts_from_passages(passages)` ‚Üí List[concept] (optionnel, pour pr√©-processing)

**Usage :**
```python
# Query enrichie avec concepts actifs du STATE
query = f"{user_input} {state_image.get('concepts_actifs', [])}"
passages = retrieve_passages(query, philosopher="spinoza", top_k=3)
# Retourne: [{"text": "...", "score": 0.85, "source": "√âthique"}, ...]
```

**Thresholds :**
- Similarity score ‚â• 0.65 : passage pertinent
- Similarity score < 0.45 : passage ignor√©

### 3. BERT Encoder

**Mod√®le :** BERT-base ou BERT-mini (CPU)

**Responsabilit√©s :**
- Encode conversation + passages RAG
- Analyse multi-axes (concepts, intention, tension, style, etc.)
- G√©n√®re STATE_IMAGE condens√© structur√©
- Extrait concepts des passages RAG (pas texte brut)

**Localisation :** `Backend/bert_encoder.py`

**Fonctions cl√©s :**
- `encode_to_state_image(conversation, rag_passages, prev_state, mini_store_feedback)` ‚Üí STATE_IMAGE
- `analyze_axes(text)` ‚Üí dict avec axes analys√©s
- `extract_concepts_from_rag(passages)` ‚Üí List[concept]

**Fr√©quence :**
- 1√ó par tour utilisateur
- Ou p√©riodiquement (tous les N tokens, heuristique)

**Chargement :**
- CPU (pas GPU)
- Mod√®le charg√© une fois, r√©utilis√©

### 4. Mistral 7B Generator

**Mod√®le :** Mistral 7B (ou √©quivalent, ex: SPS-distilled)

**Responsabilit√©s :**
- G√©n√®re r√©ponse conversationnelle
- Lit STATE_IMAGE structur√©
- Applique focus sur concepts + intention + style
- D√©tecte motifs/tics de langage pour Mini-store

**Localisation :** `Backend/mistral_generator.py`

**Fonctions cl√©s :**
- `generate(state_image, user_input, system_prompt)` ‚Üí response
- `detect_patterns(response)` ‚Üí patterns_dict (pour Mini-store)

**Contraintes :**
- Contexte ‚â§ 500 tokens
- STATE_IMAGE structur√© (~100-200 tokens)
- Dernier √©change utilisateur (~50-100 tokens)
- R√©serve pour g√©n√©ration

**Chargement :**
- GPU (vLLM ou √©quivalent)
- Mod√®le charg√© une fois, r√©utilis√©

### 5. Mini-store (Heuristique)

**Responsabilit√©s :**
- Collecte motifs r√©currents d√©tect√©s par Mistral
- Observe tics de langage, termes r√©p√©t√©s
- Fournit feedback pour pond√©ration STATE par BERT

**Localisation :** `Backend/mini_store.py`

**Fonctions cl√©s :**
- `update_patterns(pattern_type, pattern_data)` ‚Üí void
- `get_feedback_for_bert()` ‚Üí feedback_dict

**Structure :**
```python
{
    "recurrences": {
        "termes_repetes": {"conatus": 5, "affects": 3},
        "structures": ["question rh√©torique", "analogie"]
    },
    "alertes": ["ton trop formel", "r√©p√©tition excessive"]
}
```

---

## üîÑ Pipeline Fonctionnel

### Sch√©ma Complet

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇUser Input    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇRouter        ‚îÇ
‚îÇ- D√©termine   ‚îÇ
‚îÇ  philosophe  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇRAG Retriever ‚îÇ
‚îÇ- Query enrichie‚îÇ
‚îÇ- Corpus philosophe‚îÇ
‚îÇ- Top 2-3 passages‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ passages
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇBERT Encoder  ‚îÇ
‚îÇ- Encode:     ‚îÇ
‚îÇ  * conversation‚îÇ
‚îÇ  * passages RAG‚îÇ
‚îÇ  * STATE prev ‚îÇ
‚îÇ  * feedback   ‚îÇ
‚îÇ- Analyse axes ‚îÇ
‚îÇ- Extrait concepts‚îÇ
‚îÇ- Condense     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ STATE_IMAGE
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇMistral 7B    ‚îÇ
‚îÇ- Lit STATE   ‚îÇ
‚îÇ- G√©n√®re (‚â§500t)‚îÇ
‚îÇ- D√©tecte motifs‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ response + patterns
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇMini-store    ‚îÇ
‚îÇ- Collecte    ‚îÇ
‚îÇ- Feedback    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚ñ∫ BERT (prochain tour)
```

---

## üìä Mod√®les de Donn√©es

### STATE_IMAGE Schema (JSON Structur√©)

**Version Compl√®te (POC et Production)** :
```json
{
  "concepts_actifs": [
    "conatus",
    "effort pers√©v√©rer",
    "puissance d'agir"
  ],
  "concepts_rag": [
    "conatus = puissance d'agir",
    "affects modifient conatus"
  ],
  "sources_rag": [
    "Spinoza, √âthique, III, prop. 7"
  ],
  "intention": "clarifier",
  "tension": "neutre",
  "recurrence": {
    "termes_repetes": {
      "conatus": 3,
      "affects": 2
    },
    "structures": ["question rh√©torique"]
  },
  "style": "p√©dagogique",
  "ton": "bienveillant",
  "priorite": [
    "concepts_actifs",
    "intention"
  ],
  "relations": [
    "conatus ‚Üí affects",
    "cause ‚Üí effet"
  ],
  "emotion": "curieux",
  "metadata": {
    "philosopher": "spinoza",
    "turn": 5,
    "timestamp": "2025-12-10T10:30:00Z"
  }
}
```

**Note POC** : Pour maximiser la pr√©cision de la d√©mo, le STATE_IMAGE complet avec tous les axes est utilis√© d√®s le POC (pas de version r√©duite).

### Session State Schema

```json
{
  "session_id": "uuid",
  "philosopher": "spinoza",
  "current_turn": 5,
  "state_image": { /* STATE_IMAGE ci-dessus */ },
  "mini_store": {
    "recurrences": { /* ... */ },
    "alertes": [ /* ... */ ]
  },
  "dialogue_history": [
    {
      "turn": 1,
      "user": "...",
      "assistant": "...",
      "rag_passages_used": ["passage_id_1"]
    }
  ],
  "rag_cache": {
    "passage_id_1": {
      "text": "...",
      "concepts_extracted": ["conatus", "puissance"]
    }
  }
}
```

### RAG Passage Schema

```json
{
  "passage_id": "uuid",
  "text": "Le conatus est l'effort par lequel chaque chose s'efforce de pers√©v√©rer dans son √™tre...",
  "source": "Spinoza, √âthique, III, prop. 7",
  "philosopher": "spinoza",
  "embedding": [0.123, ...],
  "concepts": ["conatus", "effort", "pers√©v√©rer"],
  "similarity_score": 0.85
}
```

---

## üó∫Ô∏è Roadmap d'Impl√©mentation - POC Bootstrap 0‚Ç¨ ‚Üí 100‚Ç¨

### Objectif du POC

**Technique** : Valider que la cha√Æne compl√®te (Router ‚Üí RAG ‚Üí BERT Condensation ‚Üí Mistral G√©n√©ration) est fonctionnelle, que le `STATE_IMAGE` est correctement structur√© et lu, et que le contexte g√©n√©r√© reste sous les 500 tokens.

**Business** : Cr√©er un asset valorisable (portfolio, consulting, formation) d√©montrant une r√©duction de 50% des co√ªts d'inf√©rence LLM via architecture TRM.

**‚ö†Ô∏è Point Critique** : La gestion de la m√©moire et de la concurrence entre les deux mod√®les sur des ressources diff√©rentes (CPU pour BERT, GPU pour Mistral) doit √™tre valid√©e tr√®s t√¥t, avant m√™me l'int√©gration fonctionnelle (voir √©tape 1.1b).

### Strat√©gie Budget Minimal

**Phase 0 (0‚Ç¨)** : D√©veloppement sur Colab gratuit (√©conomie ~60‚Ç¨)  
**Phase 1 (80-100‚Ç¨)** : Infrastructure Vast.ai/RunPod uniquement pour int√©gration finale et benchmarks  
**Phase 2 (0‚Ç¨)** : Valorisation imm√©diate (articles, portfolio, consulting)

### Phase 0 : Pr√©paration 0‚Ç¨ (Cette semaine - Colab Gratuit)

**Objectif** : Maximiser la pr√©paration gratuite pendant l'attente des 100‚Ç¨.

#### ‚úÖ √âtape 0.1 : Prototypage BERT Encoder (Colab CPU Gratuit)
- [ ] Test extraction concepts + STATE_IMAGE generation
- [ ] Pas besoin GPU pour BERT (CPU Colab suffit)
- [ ] Validation logique multi-axes (concepts, intention, tension, style, etc.)
- [ ] Tests unitaires basiques

**Livrables :**
- Notebook Colab : `Backend/Notebooks/bert_encoder_prototype.ipynb`
- Fonction `extract_concepts_from_rag()` test√©e
- Fonction `encode_to_state_image()` test√©e

#### ‚úÖ √âtape 0.2 : Pr√©paration Corpus RAG (Colab T4 Gratuit)
- [ ] G√©n√©ration embeddings sentence-transformers (Colab gratuit T4)
- [ ] Indexation FAISS des 3 corpus (Spinoza/Bergson/Kant)
- [ ] Sauvegarde index localement (pour Phase 1)
- [ ] Tests retrieval basiques

**Livrables :**
- Notebook Colab : `Backend/Notebooks/rag_indexing.ipynb`
- Indexes FAISS sauvegard√©s : `Backend/data/rag_indexes/`
- Scripts de chargement index

#### ‚úÖ √âtape 0.3 : Tests Mistral 7B Baseline (Colab T4 Gratuit - Sessions 12h)
- [ ] Chargement mod√®le Mistral 7B + inf√©rence test
- [ ] Mesure latence baseline
- [ ] Validation que √ßa tient en 15GB VRAM (T4 Colab)
- [ ] Tests avec STATE_IMAGE mock

**Livrables :**
- Notebook Colab : `Backend/Notebooks/mistral_baseline.ipynb`
- M√©triques baseline : latence, VRAM usage
- Validation faisabilit√© technique

#### ‚úÖ √âtape 0.4 : Documentation & Contenus (0‚Ç¨)
- [ ] Architecture diagram (Excalidraw gratuit)
- [ ] README structure (template PRD)
- [ ] Script benchmark automatis√© (pour Phase 1)
- [ ] Pr√©paration templates articles/blog posts

**Livrables :**
- `docs/references/architecture_diagram.excalidraw`
- `README.md` structure portfolio-quality
- `Backend/scripts/benchmark_automated.py`
- Templates contenu valorisation

**Budget Phase 0 : 0‚Ç¨** (tout sur Colab gratuit)

---

### Phase 1 : POC Fonctionnel sur Budget Minimal (80-100‚Ç¨)

**Infrastructure** : Vast.ai (~$0.30-0.40/h) ou RunPod (~$0.49/h)  
**Budget** : 100‚Ç¨ = ~250h Vast.ai (10 jours 24/7) ou ~200h RunPod (8 jours 24/7)  
**Strat√©gie** : Dev sur Colab gratuit, Vast.ai uniquement pour int√©gration finale (24-48h)

#### ‚úÖ √âtape 1.1 : Router & RAG Retriever (Colab Gratuit - Jours 1-2)
- [ ] Impl√©menter Router (d√©termination philosophe)
- [ ] Int√©grer RAG Retriever avec `sentence-transformers` (embeddings d√©j√† g√©n√©r√©s Phase 0)
- [ ] Charger index FAISS pr√©comput√© (Phase 0)
- [ ] **Minimal Testing:** Test unitaire de base pour v√©rifier que `retrieve_passages` retourne des r√©sultats.

**Livrables :**
- `Backend/router.py`
- `Backend/rag_retriever.py`
- Tests unitaires basiques

**Note** : Index FAISS d√©j√† g√©n√©r√© Phase 0, pas besoin de GPU ici.

#### üÜï √âtape 1.1b : **V√©rification des Ressources & Chargement** (Vast.ai - Jour 4, ~20‚Ç¨) ‚ö†Ô∏è CRITIQUE

**Objectif** : Valider que les mod√®les peuvent √™tre charg√©s simultan√©ment sans conflit de ressources (m√©moire, ports, acc√®s) avant d'impl√©menter la logique fonctionnelle.

**Infrastructure** : Vast.ai RTX 4090 (~$0.30/h) + CPU instance (~$0.05/h) = ~$0.35/h  
**Dur√©e estim√©e** : 6-8h = ~20‚Ç¨

- [ ] **Chargement BERT (CPU)** :
  - [ ] Charger le mod√®le BERT-base/mini sur le **CPU**
  - [ ] V√©rifier l'empreinte m√©moire RAM (doit rester < 4GB pour BERT-mini)
  - [ ] Ex√©cuter une inf√©rence test pour valider l'acc√®s
  - [ ] Documenter la latence d'inf√©rence basique

- [ ] **Chargement Mistral (GPU)** :
  - [ ] Charger le mod√®le Mistral 7B sur le **GPU** via vLLM/PyTorch
  - [ ] V√©rifier la VRAM disponible (√©viter OOM - Object Out of Memory)
  - [ ] V√©rifier que le mod√®le reste charg√© en m√©moire GPU
  - [ ] Ex√©cuter une inf√©rence test pour valider l'acc√®s
  - [ ] Documenter la latence d'inf√©rence basique

- [ ] **Test de Coexistence** :
  - [ ] Ex√©cuter une inf√©rence BERT (CPU) et une inf√©rence Mistral (GPU) **simultan√©ment**
  - [ ] V√©rifier qu'il n'y a **pas de conflits** d'acc√®s aux ressources
  - [ ] V√©rifier qu'il n'y a **pas de d√©gradation critique** de la latence lors du chargement simultan√©
  - [ ] Valider que les deux mod√®les restent accessibles apr√®s coexistence

**Livrables :**
- `Backend/tests/test_model_loading.py` (tests de chargement)
- `Backend/tests/test_resource_coexistence.py` (tests de coexistence)
- Rapport de v√©rification ressources (m√©moire RAM, VRAM, latences)
- Script de validation ressources : `Backend/scripts/validate_resources.py`

**Justification** : Cette √©tape s√©pare les risques li√©s au **d√©ploiement** (chargement des mod√®les) des risques li√©s au **fonctionnel** (impl√©mentation de la logique d'encodage et de g√©n√©ration), r√©duisant ainsi le temps de d√©bogage de la Phase 2.

#### ‚úÖ √âtape 1.2 : BERT Encoder Core Complet (Colab Gratuit - Jour 2-3)
- [ ] Impl√©menter classe `BERTEncoder` (CPU) - **prototype d√©j√† test√© Phase 0**
- [ ] **Analyse Multi-Axes Compl√®te:** Impl√©menter l'analyse de tous les axes pour maximiser la pr√©cision du POC :
  - `concepts_actifs` : Id√©es centrales, d√©cisions r√©centes
  - `concepts_rag` : Concepts extraits des passages RAG
  - `intention` : Question, clarification, instruction
  - `tension` : Accord / d√©saccord / neutre
  - `style` : Concis, humoristique, p√©dagogique
  - `ton` : Bienveillant, formel, etc.
  - `priorite` : Crucial vs secondaire
  - `relations` : Cause ‚Üí effet, analogies
  - `emotion` : Politesse, ironie, frustration
- [ ] Fonction `extract_concepts_from_rag()` (extraction concepts, **critique pour le POC**)
- [ ] Fonction `encode_to_state_image()` avec tous les axes
- [ ] **Testing:** Test unitaire pour v√©rifier que le `STATE_IMAGE` g√©n√©r√© n'inclut **pas de texte brut des passages RAG** et respecte la structure JSON compl√®te.

**Livrables :**
- `Backend/bert_encoder.py` (avec analyse multi-axes compl√®te)
- Tests unitaires basiques (focus validation structure STATE_IMAGE compl√®te)

**Note** : Le mod√®le BERT est d√©j√† charg√© et valid√© depuis l'√©tape 1.1b, permettant de se concentrer sur la logique fonctionnelle.

#### ‚úÖ √âtape 1.3 : Mistral Generator & Mini-store (Colab Gratuit - Jour 3)
- [ ] Impl√©menter classe `MistralGenerator` (GPU) - **baseline d√©j√† test√©e Phase 0**
- [ ] Fonction `generate()` lisant le `STATE_IMAGE` et l'`user_input` (Contexte ‚â§ 500 tokens)
- [ ] **Mini-store Simplifi√©/Report√©:** Mettre en place la classe `MiniStore`, mais **d√©sactiver ou simplifier** la boucle de feedback vers BERT. La d√©tection de patterns peut √™tre **report√©e** apr√®s la validation de la cha√Æne principale.
- [ ] **Minimal Testing:** Test unitaire pour v√©rifier que `generate` produit une r√©ponse coh√©rente en utilisant le `STATE_IMAGE`.

**Livrables :**
- `Backend/mistral_generator.py`
- `Backend/mini_store.py` (structure de base, feedback d√©sactiv√©)
- Tests g√©n√©ration basiques

**Note** : Le mod√®le Mistral est d√©j√† charg√© et valid√© depuis l'√©tape 1.1b, permettant de se concentrer sur la logique fonctionnelle de g√©n√©ration.

#### ‚úÖ √âtape 1.4 : Templates & Endpoint (Colab Gratuit - Jour 3)
- [ ] Templates Prompts (System prompt + injection `STATE_IMAGE`)
- [ ] **Endpoint `/chat_trm` (Premi√®re Int√©gration):** Cr√©er l'endpoint FastAPI int√©grant la cha√Æne compl√®te pour une seule requ√™te.
- [ ] **Session Management Basique:** Stockage session/√©tat minimaliste (in-memory).

**Livrables :**
- `Backend/prompts/system_prompt.py`
- `Backend/prompts/chat_template.py`
- `Backend/api/endpoints/trm.py`
- `Backend/session_manager.py` (basique)

---

### Phase 2 : Int√©gration & Benchmarks sur Vast.ai (Jours 4-7, ~60‚Ç¨)

**Infrastructure** : Vast.ai RTX 4090 + CPU  
**Budget** : ~60‚Ç¨ (24-48h d'utilisation)

#### ‚úÖ √âtape 2.1 : Int√©gration Compl√®te (Vast.ai - Jour 4-5, ~40‚Ç¨)
- [ ] D√©ployer pipeline complet sur Vast.ai (2 instances : GPU + CPU)
- [ ] Finaliser l'int√©gration compl√®te du pipeline (Router ‚Üí RAG ‚Üí BERT ‚Üí Mistral).
- [ ] G√©rer la persistance du `STATE_IMAGE` entre les tours de conversation (Session State).
- [ ] **Tests d'Int√©gration Critiques :** Simuler 5 dialogues complets (end-to-end) pour s'assurer que le `STATE_IMAGE` s'enrichit correctement √† chaque tour.
- [ ] Ajustements/debugging

**Livrables :**
- Pipeline complet fonctionnel d√©ploy√©
- `tests/test_integration_trm.py` (5 dialogues de test)
- Rapport validation cha√Æne

#### ‚úÖ √âtape 2.2 : Benchmarks Comparatifs (Vast.ai - Jour 6-7, ~40‚Ç¨)
- [ ] **Benchmarks Comparatifs** : TRM vs Qwen 14B simul√© (baseline)
- [ ] **M√©triques Mesur√©es** :
  - Co√ªt par requ√™te (GPU heures √ó prix)
  - Latence moyenne (ms)
  - Qualit√© r√©ponses (subjective ou score automatique)
  - Throughput (req/s)
- [ ] **Focus Qualit√©/Taille:** Affiner l'extraction des concepts RAG pour garantir la densit√© de l'information.
- [ ] V√©rifier la taille du contexte de Mistral √† chaque tour (**v√©rification critique POC : Total ‚â§ 500 tokens**).
- [ ] Ajuster les *thresholds* RAG pour garantir la pertinence.
- [ ] Enregistrement d√©mo vid√©o (3-5 min)

**Livrables :**
- `docs/benchmarks/comparison_trm_vs_qwen14b.md`
- Graphiques co√ªt/latence/qualit√©
- Fonction `extract_concepts_from_rag()` affin√©e
- `Backend/config/thresholds.py` (thresholds optimaux)
- Validation taille contexte ‚â§ 500 tokens
- Vid√©o d√©mo POC (3-5 min)

**Budget Phase 2** : ~60‚Ç¨ (40‚Ç¨ int√©gration + 20‚Ç¨ benchmarks)

---

### Phase 3 : Valorisation Imm√©diate (0‚Ç¨ - Semaines Post-POC)

**Objectif** : Transformer POC en assets mon√©tisables

#### ‚úÖ √âtape 3.1 : Contenu & Visibilit√© (Semaine 1 Post-POC)
- [ ] **Article Medium/Dev.to** : "TRM Architecture: 50% Cost Reduction for LLM Inference"
  - Inclure benchmarks chiffr√©s
  - Code snippets cl√©s
  - Architecture diagram
  - Target : 2000-3000 mots
- [ ] **GitHub Repo Public** : `bergsonAndFriends-TRM`
  - README portfolio-quality
  - Benchmarks dans `/docs/benchmarks/`
  - Scripts reproduction
  - Architecture diagram
- [ ] **LinkedIn S√©rie** (5 posts) :
  - Post 1 : Probl√®me (co√ªts inf√©rence LLM explosifs)
  - Post 2 : Solution (architecture TRM - sch√©ma)
  - Post 3 : Impl√©mentation (challenges techniques)
  - Post 4 : R√©sultats (benchmarks -50% co√ªt)
  - Post 5 : Ouverture (code open source + retours)
- [ ] **Proposition Talks** : PyConFR, MLOps meetups, HuggingFace meetups
  - Titre : "From 14B to 7B: How BERT+Mistral Saved Our LLM Costs"

**Livrables :**
- Article Medium publi√© (1000+ views target)
- Repo GitHub public (50+ stars target)
- 5 posts LinkedIn (100+ reactions target)
- 1-2 propositions talks accept√©es

#### ‚úÖ √âtape 3.2 : Produits D√©riv√©s (Semaine 2-3 Post-POC)
- [ ] **Template Migration TRM** : Guide + scripts
- [ ] **Vid√©o YouTube** (15-20 min) :
  - Walkthrough architecture
  - D√©mo live POC
  - Benchmarks comment√©s
- [ ] **Consulting Pitch Deck** : "LLM Cost Audit"
- [ ] **Formation/Tutoriel** (si traction audience) :
  - Cours "LLM Cost Optimization with TRM Architecture"
  - Prix : 49-99‚Ç¨
  - Plateforme : Udemy, Gumroad, ou propre site

**Livrables :**
- Template migration TRM
- Vid√©o YouTube (500+ views target)
- Pitch deck consulting
- Formation pr√©vente (si demande)

**ROI Cible** : 100‚Ç¨ investis ‚Üí 2-5k‚Ç¨ retour (√ó20-50) via :
- Consulting : 1-2 missions (5-10k‚Ç¨)
- Formation : Pr√©vente tutoriel (49‚Ç¨ √ó 20-50 personnes = 1-2.5k‚Ç¨)
- Sponsoring : GitHub Sponsors (5-10$/mois √ó 10-20 early adopters)

---

## üìä Analyse Co√ªt/B√©n√©fice

### Investissement POC TRM

| Poste | Co√ªt | Dur√©e |
|-------|------|-------|
| Temps d√©veloppement | 2-3 semaines (votre temps) | - |
| Infrastructure Vast.ai/RunPod | 80-100‚Ç¨ (tests + benchmarks) | 1 mois |
| **Total** | **~100‚Ç¨ + temps** | **3 semaines** |

### Retours Potentiels (6 mois)

| Sc√©nario | Probabilit√© | Retour Estim√© |
|----------|------------|---------------|
| Consulting (1-2 missions) | 30% | 5-10k‚Ç¨ |
| Formation/Tutoriel | 20% | 1-3k‚Ç¨ (ventes) |
| Sponsors Open Source | 10% | 500-1k‚Ç¨/an |
| Visibilit√© ‚Üí Opportunit√©s | 50% | Inestimable (jobs, network) |
| Rien (mais apprentissage) | 20% | Expertise TRM (valorisable plus tard) |

**ROI Conservateur** : M√™me si 0‚Ç¨ direct, vous avez :
- Portfolio technique solide
- Expertise diff√©renciante (TRM avant la hype)
- Contenu r√©utilisable (articles, talks, d√©mos)

#### ‚úÖ √âtape 2.1 : Int√©gration Compl√®te (1 jour)
- [ ] Finaliser l'int√©gration compl√®te du pipeline (Router ‚Üí RAG ‚Üí BERT ‚Üí Mistral).
- [ ] G√©rer la persistance du `STATE_IMAGE` entre les tours de conversation (Session State).
- [ ] **Tests d'Int√©gration Critiques :** Simuler 5 dialogues complets (end-to-end) pour s'assurer que le `STATE_IMAGE` s'enrichit correctement √† chaque tour.

**Livrables :**
- Pipeline complet fonctionnel
- `tests/test_integration_trm.py` (5 dialogues de test)
- Rapport validation cha√Æne

#### ‚úÖ √âtape 2.2 : Optimisation de la Densit√© (1 jour)
- [ ] **Focus Qualit√©/Taille:** Affiner l'extraction des concepts RAG pour garantir la densit√© de l'information.
- [ ] V√©rifier la taille du contexte de Mistral √† chaque tour (**v√©rification critique POC : Total ‚â§ 500 tokens**).
- [ ] Ajuster les *thresholds* RAG pour garantir la pertinence.

**Livrables :**
- Fonction `extract_concepts_from_rag()` affin√©e
- `Backend/config/thresholds.py` (thresholds optimaux)
- Validation taille contexte ‚â§ 500 tokens
- Rapport optimisation densit√©

### Phase 4 : Am√©liorations & Report√©s (Post-Valorisation)

| √âtape Report√©e | Description et Objectif |
| :--- | :--- |
| **Phase 3.1 & 3.3 (Mini-store)** | Activation compl√®te du `Mini-store`, affinage de la d√©tection de patterns et impl√©mentation de la boucle de feedback vers BERT. |
| **Phase 4.3 (Tests de Charge)** | Simuler sessions parall√®les, v√©rifier latence moyenne < 3s, et s'assurer qu'il n'y a pas d'erreurs de m√©moire. |
| **Phase 4.4 (√âvaluation Qualit√©)** | √âvaluation formelle de la coh√©rence et du style g√©n√©r√© par rapport √† une *baseline* (n√©cessite un dataset d'√©valuation). |
| **Optimisation Pipeline (Parall√©lisation)** | Mise en cache et parall√©lisation RAG/BERT pour la mont√©e en charge. |
| **Multi-philosophes** | Extension du corpus √† Kant et Bergson (actuellement Spinoza uniquement). |
| **Note** | Les axes multiples BERT sont **d√©j√† impl√©ment√©s** dans le POC pour maximiser la pr√©cision de la d√©mo. |

---

**Note POC :** Cet ajustement permet de d√©bloquer rapidement la preuve de concept en **3.5 √† 4 jours** en se concentrant sur les fonctions de base, tout en laissant le travail d'optimisation, de performance et d'heuristique (Mini-store) pour apr√®s la validation initiale.

---

## üìà M√©triques & Tests - POC

### M√©triques Critiques pour POC

1. **Validation Cha√Æne Fonctionnelle** :
   - ‚úÖ Pipeline complet ex√©cutable sans erreur
   - ‚úÖ STATE_IMAGE g√©n√©r√© et structur√© correctement
   - ‚úÖ Contexte Mistral ‚â§ 500 tokens (v√©rification critique)

2. **Qualit√© STATE_IMAGE (POC Complet)** :
   - Nombre concepts extraits : 3-8 par tour
   - Taille STATE_IMAGE : 150-250 tokens (version compl√®te avec tous les axes)
   - **Critique POC :** Pas de texte brut des passages RAG dans STATE_IMAGE
   - **Critique POC :** Concepts RAG pr√©sents dans `concepts_rag` (condens√©s)
   - **Critique POC :** Tous les axes analys√©s (concepts_actifs, intention, tension, style, ton, priorit√©, relations, emotion)
   - Couverture axes : 9/9 axes analys√©s (maximum pr√©cision pour d√©mo)

3. **Qualit√© G√©n√©ration (POC Complet)** :
   - R√©ponse g√©n√©r√©e coh√©rente avec STATE_IMAGE complet
   - **Critique POC :** Pas de passages RAG bruts dans r√©ponse g√©n√©r√©e
   - Utilisation des concepts du STATE_IMAGE dans la r√©ponse
   - Utilisation de tous les axes du STATE_IMAGE (style, ton, intention, relations, etc.)

4. **Efficacit√© RAG (POC)** :
   - Similarity score moyen : ‚â• 0.45 (threshold minimal)
   - Passages r√©cup√©r√©s pertinents (v√©rification manuelle)

### M√©triques Report√©es (Post-POC)

- Latence d√©taill√©e par composant
- √âvaluation formelle qualit√©/style
- Tests de charge et performance
- M√©triques Mini-store (d√©sactiv√© en POC)

### Tests Critiques pour POC

**Test `rag_retriever.retrieve_passages()`** :
```python
def test_retrieve_passages():
    retriever = RAGRetriever(corpus="spinoza")
    query = "Qu'est-ce que le conatus ?"
    passages = retriever.retrieve_passages(query, top_k=3)
    
    assert len(passages) <= 3
    assert all(p['similarity_score'] >= 0.45 for p in passages)
    assert all('conatus' in p['text'].lower() or 'conatus' in p['concepts'] 
                for p in passages[:2])  # Top 2 doivent √™tre pertinents
```

**Test `bert_encoder.extract_concepts_from_rag()`** :
```python
def test_extract_concepts_from_rag():
    encoder = BERTEncoder()
    passages = [
        {"text": "Le conatus est l'effort pour pers√©v√©rer...", "concepts": ["conatus"]}
    ]
    concepts = encoder.extract_concepts_from_rag(passages)
    
    assert len(concepts) > 0
    assert "conatus" in concepts
    # V√©rifier que concepts sont condens√©s, pas texte brut
    assert all(len(c) < 50 for c in concepts)  # Concepts courts
```

**Test `bert_encoder.encode_to_state_image()`** :
```python
def test_encode_to_state_image():
    encoder = BERTEncoder()
    conversation = [{"user": "...", "assistant": "..."}]
    rag_passages = [{"text": "...", "concepts": ["conatus"]}]
    prev_state = None
    
    state_image = encoder.encode_to_state_image(
        conversation, rag_passages, prev_state, {}
    )
    
    assert "concepts_actifs" in state_image
    assert "concepts_rag" in state_image
    assert len(state_image["concepts_rag"]) > 0
    assert state_image["concepts_rag"][0] != rag_passages[0]["text"]  # Pas texte brut
```

**Test `mistral_generator.generate()`** :
```python
def test_generate_with_state_image():
    generator = MistralGenerator()
    state_image = {
        "concepts_actifs": ["conatus"],
        "intention": "clarifier",
        "style": "p√©dagogique"
    }
    user_input = "Peux-tu m'expliquer le conatus ?"
    
    response = generator.generate(state_image, user_input, system_prompt)
    
    assert len(response) > 0
    # V√©rifier que r√©ponse utilise concepts de STATE_IMAGE
    assert "conatus" in response.lower()
    # V√©rifier que pas de passages RAG bruts
    assert "√âthique" not in response or "prop." not in response  # Pas citations brutes
```

### Tests d'Int√©gration (POC - 5 Dialogues Minimum)

**Test pipeline complet** :
```python
def test_pipeline_complete():
    # Simuler dialogue complet
    session = SessionManager.create_session(philosopher="spinoza")
    
    for turn in range(5):
        user_input = f"Message tour {turn}"
        
        # Pipeline complet
        philosopher = router.determine_philosopher(user_input, session.context)
        passages = rag_retriever.retrieve_passages(user_input, philosopher)
        state_image = bert_encoder.encode_to_state_image(
            session.dialogue_history, passages, session.state_image, session.mini_store.get_feedback()
        )
        response = mistral_generator.generate(state_image, user_input, system_prompt)
        patterns = mistral_generator.detect_patterns(response)
        mini_store.update_patterns(patterns)
        
        # V√©rifications Critiques POC
        assert state_image["philosopher"] == "spinoza"
        assert len(state_image["concepts_rag"]) > 0
        assert len(response) > 0
        # V√©rifier pas de texte brut RAG dans STATE_IMAGE
        assert not any("√âthique" in str(v) or "prop." in str(v) 
                      for v in state_image.values() if isinstance(v, (str, list)))
        # V√©rifier taille contexte ‚â§ 500 tokens (approximatif)
        context_tokens = len(state_image_str.split()) + len(user_input.split())
        assert context_tokens <= 500, f"Contexte trop long: {context_tokens} tokens"
```

---

## üíª Exemples Concrets

### V√©rification Ressources & Chargement Mod√®les (√âtape 1.1b)

```python
import torch
from transformers import AutoTokenizer, AutoModel
from vllm import LLM
import psutil
import GPUtil

def validate_resources():
    """
    Valide que les mod√®les peuvent √™tre charg√©s simultan√©ment
    sans conflit de ressources.
    """
    results = {
        "bert_cpu": {},
        "mistral_gpu": {},
        "coexistence": {}
    }
    
    # 1. Chargement BERT sur CPU
    print("Chargement BERT sur CPU...")
    bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    bert_model = AutoModel.from_pretrained("bert-base-uncased")
    bert_model.eval()
    
    # V√©rifier m√©moire RAM
    ram_before = psutil.virtual_memory().used / (1024**3)  # GB
    ram_after = psutil.virtual_memory().used / (1024**3)
    ram_bert = ram_after - ram_before
    
    results["bert_cpu"]["ram_usage_gb"] = ram_bert
    results["bert_cpu"]["ram_ok"] = ram_bert < 4.0  # < 4GB pour BERT-mini
    
    # Test inf√©rence BERT
    test_input = bert_tokenizer("Test BERT", return_tensors="pt")
    with torch.no_grad():
        output = bert_model(**test_input)
    results["bert_cpu"]["inference_ok"] = True
    results["bert_cpu"]["latency_ms"] = 50  # Exemple
    
    # 2. Chargement Mistral sur GPU
    print("Chargement Mistral sur GPU...")
    gpu_before = GPUtil.getGPUs()[0].memoryUsed if GPUtil.getGPUs() else 0
    
    mistral_llm = LLM(
        model="mistralai/Mistral-7B-Instruct-v0.2",
        tensor_parallel_size=1,
        gpu_memory_utilization=0.8
    )
    
    gpu_after = GPUtil.getGPUs()[0].memoryUsed if GPUtil.getGPUs() else 0
    vram_mistral = gpu_after - gpu_before
    
    results["mistral_gpu"]["vram_usage_gb"] = vram_mistral / 1024
    results["mistral_gpu"]["vram_ok"] = vram_mistral < 16 * 1024  # < 16GB VRAM
    
    # Test inf√©rence Mistral
    test_prompt = "Test Mistral"
    outputs = mistral_llm.generate([test_prompt], SamplingParams(max_tokens=10))
    results["mistral_gpu"]["inference_ok"] = True
    results["mistral_gpu"]["latency_ms"] = 200  # Exemple
    
    # 3. Test de coexistence simultan√©e
    print("Test coexistence simultan√©e...")
    import time
    
    start = time.time()
    
    # Inf√©rence BERT (CPU) et Mistral (GPU) simultan√©ment
    bert_input = bert_tokenizer("Test coexistence BERT", return_tensors="pt")
    mistral_outputs = mistral_llm.generate(["Test coexistence Mistral"], 
                                          SamplingParams(max_tokens=10))
    
    with torch.no_grad():
        bert_output = bert_model(**bert_input)
    
    elapsed = time.time() - start
    
    results["coexistence"]["simultaneous_ok"] = True
    results["coexistence"]["latency_ms"] = elapsed * 1000
    results["coexistence"]["no_conflicts"] = True  # Pas d'erreur
    
    # V√©rifications finales
    results["coexistence"]["both_accessible"] = (
        results["bert_cpu"]["inference_ok"] and 
        results["mistral_gpu"]["inference_ok"]
    )
    
    return results

# Ex√©cution
if __name__ == "__main__":
    results = validate_resources()
    print("\n=== R√©sultats Validation Ressources ===")
    print(f"BERT CPU - RAM: {results['bert_cpu']['ram_usage_gb']:.2f} GB - OK: {results['bert_cpu']['ram_ok']}")
    print(f"Mistral GPU - VRAM: {results['mistral_gpu']['vram_usage_gb']:.2f} GB - OK: {results['mistral_gpu']['vram_ok']}")
    print(f"Coexistence - Latence: {results['coexistence']['latency_ms']:.0f} ms - OK: {results['coexistence']['no_conflicts']}")
    
    if all([
        results['bert_cpu']['ram_ok'],
        results['mistral_gpu']['vram_ok'],
        results['coexistence']['no_conflicts'],
        results['coexistence']['both_accessible']
    ]):
        print("\n‚úÖ Validation r√©ussie - Les mod√®les peuvent coexister")
    else:
        print("\n‚ùå Validation √©chou√©e - V√©rifier les ressources")
```

### RAG Retrieve avec Enrichissement Query

```python
from sentence_transformers import SentenceTransformer, util
import faiss
import numpy as np

class RAGRetriever:
    def __init__(self, corpus_path, philosopher):
        self.embedder = SentenceTransformer("all-MiniLM-L6-v2")
        self.philosopher = philosopher
        self.index = self._load_index(corpus_path)
        self.passages = self._load_passages(corpus_path)
    
    def retrieve_passages(self, user_input, state_image=None, top_k=3):
        # Enrichir query avec concepts actifs du STATE
        if state_image and state_image.get("concepts_actifs"):
            query = f"{user_input} {' '.join(state_image['concepts_actifs'][:3])}"
        else:
            query = user_input
        
        # Encode query
        query_emb = self.embedder.encode(query, convert_to_tensor=True)
        
        # Search
        scores, indices = self.index.search(
            query_emb.cpu().numpy().reshape(1, -1), top_k
        )
        
        # Filter by threshold
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if score >= 0.45:  # Threshold
                passage = self.passages[idx]
                passage["similarity_score"] = float(score)
                results.append(passage)
        
        return results[:top_k]
```

### BERT Encode avec Extraction Concepts RAG

```python
from transformers import AutoTokenizer, AutoModel
import torch

class BERTEncoder:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.model = AutoModel.from_pretrained("bert-base-uncased")
        self.model.eval()
    
    def extract_concepts_from_rag(self, passages):
        """
        Extrait concepts cl√©s des passages RAG, pas texte brut.
        Utilise extraction d'entit√©s ou keywords.
        """
        concepts = []
        for passage in passages:
            # M√©thode 1: Utiliser concepts pr√©-annot√©s si disponibles
            if passage.get("concepts"):
                concepts.extend(passage["concepts"])
            else:
                # M√©thode 2: Extraction keywords (exemple simplifi√©)
                text = passage["text"]
                # Extraire noms propres, termes techniques
                keywords = self._extract_keywords(text)
                concepts.extend(keywords[:3])  # Top 3 par passage
        
        # D√©dupliquer et limiter
        return list(set(concepts))[:8]  # Max 8 concepts
    
    def encode_to_state_image(self, conversation, rag_passages, prev_state, mini_store_feedback):
        """
        Encode conversation + passages RAG en STATE_IMAGE structur√©.
        """
        # Extraire concepts RAG (pas texte brut)
        concepts_rag = self.extract_concepts_from_rag(rag_passages)
        sources_rag = [p.get("source", "") for p in rag_passages]
        
        # Analyser conversation
        last_exchange = conversation[-1] if conversation else {}
        user_text = last_exchange.get("user", "")
        assistant_text = last_exchange.get("assistant", "")
        
        # Analyse axes (simplifi√©, √† am√©liorer)
        axes = self.analyze_axes(user_text + " " + assistant_text)
        
        # Construire STATE_IMAGE
        state_image = {
            "concepts_actifs": axes.get("concepts", []),
            "concepts_rag": concepts_rag,
            "sources_rag": sources_rag,
            "intention": axes.get("intention", "neutre"),
            "tension": axes.get("tension", "neutre"),
            "style": axes.get("style", "neutre"),
            "priorite": axes.get("priorite", []),
            "relations": axes.get("relations", []),
            "emotion": axes.get("emotion", "neutre"),
            "recurrence": mini_store_feedback.get("recurrences", {}),
            "metadata": {
                "philosopher": rag_passages[0].get("philosopher") if rag_passages else None,
                "turn": (prev_state.get("metadata", {}).get("turn", 0) + 1) if prev_state else 1
            }
        }
        
        # Fusionner avec STATE pr√©c√©dent si existe
        if prev_state:
            state_image = self._merge_states(prev_state, state_image)
        
        return state_image
```

### Mistral Generate avec STATE_IMAGE

```python
from vllm import LLM, SamplingParams

class MistralGenerator:
    def __init__(self):
        self.llm = LLM(model="mistralai/Mistral-7B-Instruct-v0.2")
        self.sampling_params = SamplingParams(temperature=0.7, max_tokens=500)
    
    def generate(self, state_image, user_input, system_prompt):
        """
        G√©n√®re r√©ponse avec STATE_IMAGE structur√©.
        """
        # Construire prompt avec STATE_IMAGE
        prompt = self._construct_prompt(state_image, user_input, system_prompt)
        
        # V√©rifier taille contexte (‚â§500 tokens)
        tokens = self._count_tokens(prompt)
        if tokens > 500:
            # R√©duire STATE_IMAGE si n√©cessaire
            state_image = self._reduce_state_image(state_image, target_tokens=200)
            prompt = self._construct_prompt(state_image, user_input, system_prompt)
        
        # G√©n√©rer
        outputs = self.llm.generate([prompt], self.sampling_params)
        response = outputs[0].outputs[0].text
        
        return response
    
    def _construct_prompt(self, state_image, user_input, system_prompt):
        """
        Construit prompt avec STATE_IMAGE structur√©.
        """
        # Formater STATE_IMAGE en texte lisible
        state_text = self._format_state_image(state_image)
        
        prompt = f"""{system_prompt}

[CONTEXT_STATE]
{state_text}

[USER_INPUT]
{user_input}

[ASSISTANT_RESPONSE]
"""
        return prompt
    
    def _format_state_image(self, state_image):
        """
        Formate STATE_IMAGE en texte structur√© pour prompt.
        """
        lines = []
        
        if state_image.get("concepts_actifs"):
            lines.append(f"Concepts actifs: {', '.join(state_image['concepts_actifs'][:5])}")
        
        if state_image.get("concepts_rag"):
            lines.append(f"Concepts pertinents (corpus): {', '.join(state_image['concepts_rag'][:5])}")
        
        if state_image.get("intention"):
            lines.append(f"Intention: {state_image['intention']}")
        
        if state_image.get("style"):
            lines.append(f"Style: {state_image['style']}")
        
        return "\n".join(lines)
```

### Mini-store avec D√©tection Patterns

```python
from collections import Counter
import re

class MiniStore:
    def __init__(self):
        self.recurrences = {
            "termes_repetes": Counter(),
            "structures": []
        }
        self.alertes = []
    
    def update_patterns(self, response_text):
        """
        Met √† jour patterns d√©tect√©s depuis r√©ponse Mistral.
        """
        # D√©tecter termes r√©p√©t√©s
        words = re.findall(r'\b\w+\b', response_text.lower())
        common_words = [w for w, count in Counter(words).most_common(5) if count >= 2]
        self.recurrences["termes_repetes"].update(common_words)
        
        # D√©tecter structures
        if "?" in response_text:
            self.recurrences["structures"].append("question")
        if "car" in response_text.lower() or "parce que" in response_text.lower():
            self.recurrences["structures"].append("explication")
        
        # Alertes
        if len(common_words) > 3:
            self.alertes.append("r√©p√©tition excessive")
    
    def get_feedback_for_bert(self):
        """
        Retourne feedback pour BERT (pond√©ration STATE).
        """
        return {
            "recurrences": {
                "termes_repetes": dict(self.recurrences["termes_repetes"].most_common(5)),
                "structures": list(set(self.recurrences["structures"][-10:]))
            },
            "alertes": self.alertes[-5:]
        }
```

---

## üìù Checklist Impl√©mentation - POC Bootstrap

### Phase 0 : Pr√©paration 0‚Ç¨ (Cette semaine - Colab Gratuit)
- [ ] Prototyper BERT encoder sur Colab gratuit (CPU)
- [ ] G√©n√©rer embeddings RAG (sentence-transformers) sur Colab T4
- [ ] Tester Mistral 7B inference sur Colab gratuit (12h sessions)
- [ ] Cr√©er architecture diagram (Excalidraw)
- [ ] Pr√©parer scripts benchmark automatiques
- [ ] R√©diger README structure POC

### Phase 1 : D√©veloppement Colab Gratuit (Jours 1-3, 0‚Ç¨)
- [ ] Router & RAG Retriever (indexes Phase 0)
- [ ] BERT Encoder Core Complet (prototype Phase 0)
- [ ] Mistral Generator (baseline Phase 0)
- [ ] Templates & Endpoint `/chat_trm`

### Phase 2 : Int√©gration & Benchmarks Vast.ai (Jours 4-7, ~80‚Ç¨)
- [ ] **üÜï V√©rification Ressources & Chargement Mod√®les** (CRITIQUE - Jour 4, ~20‚Ç¨)
- [ ] Int√©gration compl√®te pipeline (Jours 4-5, ~40‚Ç¨)
- [ ] Tests d'int√©gration (5 dialogues)
- [ ] Benchmarks comparatifs TRM vs Qwen 14B (Jours 6-7, ~40‚Ç¨)
- [ ] Validation contexte ‚â§ 500 tokens
- [ ] Enregistrement d√©mo vid√©o

### Phase 3 : Valorisation Imm√©diate (Semaines Post-POC, 0‚Ç¨)
- [ ] Article Medium/Dev.to publi√©
- [ ] Repo GitHub public portfolio-quality
- [ ] S√©rie LinkedIn (5 posts)
- [ ] Proposition talks conf√©rences
- [ ] Vid√©o YouTube walkthrough
- [ ] Template migration TRM
- [ ] Consulting pitch deck

### Report√© Post-POC
- [ ] Mini-store complet avec feedback BERT
- [ ] Tests de charge
- [ ] √âvaluation qualit√© formelle
- [ ] Optimisation pipeline (parall√©lisation, cache)
- [ ] Extension multi-philosophes (Kant, Bergson)
- **Note** : Les axes multiples BERT sont **d√©j√† impl√©ment√©s** dans le POC pour maximiser la pr√©cision

---

## üöÄ Prochaines √âtapes - POC Bootstrap

### Actions Imm√©diates (Avant 100‚Ç¨)

1. **Phase 0 - Pr√©paration Colab Gratuit** (Cette semaine)
   - Prototyper BERT encoder (CPU Colab)
   - G√©n√©rer embeddings RAG (T4 Colab)
   - Tester Mistral 7B baseline (T4 Colab)
   - Cr√©er architecture diagram
   - Pr√©parer scripts benchmark

2. **Comparer Infrastructure** (Cette semaine)
   - Comparer prix Vast.ai vs RunPod pour setup final
   - D√©finir m√©triques benchmarks (co√ªt, latence, qualit√©)
   - Pr√©parer scripts d√©ploiement Vast.ai

3. **Amasser 100‚Ç¨** (Objectif avant Phase 1)

### Timeline POC Compl√®te

**Semaine 1** : Phase 0 (Colab gratuit)  
**Semaine 2** : Phase 1 (Colab gratuit) + Phase 2 d√©but (Vast.ai)  
**Semaine 3** : Phase 2 fin (benchmarks) + Phase 3 d√©but (valorisation)

**Budget Total** : 80-100‚Ç¨ (Vast.ai uniquement pour int√©gration/benchmarks)  
**ROI Cible** : 100‚Ç¨ ‚Üí 2-5k‚Ç¨ (√ó20-50) via consulting/formation/sponsoring

---

## üìö R√©f√©rences

- Architecture Dialecthon : `/Dialecthon/docs/guides/GUIDE_DIALECTHON.md`
- RAG Integration : Documentation RAG existante
- Mistral 7B : Documentation HuggingFace

---

**Note** : Ce document est vivant. Mettre √† jour au fur et √† mesure de l'impl√©mentation.
