{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ TRM POC - Notebook 4: Int√©gration Compl√®te + Dialogue Interactif\n",
    "\n",
    "**Objectif:** Pipeline complet BERT + RAG + Mistral 7B avec interface de dialogue\n",
    "\n",
    "**Runtime:** GPU Colab gratuit (T4 - 15GB VRAM)\n",
    "\n",
    "**Dur√©e estim√©e:** 3-4h (chargement + dialogue)\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 0 - POC TRM (0‚Ç¨)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Ce notebook charge **tous les composants simultan√©ment** :\n",
    "- BERT Encoder (CPU) + Mistral 7B (GPU) = ~10-12 GB VRAM\n",
    "- N√©cessite **GPU T4** activ√©\n",
    "- N√©cessite **fichiers des Notebooks 1-2-3** (code r√©utilis√©)\n",
    "\n",
    "Ce notebook impl√©mente:\n",
    "1. Import code des 3 notebooks pr√©c√©dents\n",
    "2. Chargement pipeline complet (BERT + RAG + Mistral)\n",
    "3. Interface dialogue interactive Gradio\n",
    "4. Tests end-to-end avec Spinoza\n",
    "\n",
    "**Note:** Si OOM ‚Üí R√©duire √† Mistral seul (sans BERT) pour tests basiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Pr√©requis\n",
    "\n",
    "**Avant de commencer, vous devez avoir :**\n",
    "\n",
    "1. ‚úÖ **Ex√©cut√© Notebook 2** (RAG Embeddings) et t√©l√©charg√© `rag_exports.zip`\n",
    "2. ‚úÖ **Upload√© `rag_exports.zip`** dans ce notebook (ou re-g√©n√©rer embeddings)\n",
    "3. ‚úÖ **Activ√© GPU T4** : Runtime > Change runtime type > T4 GPU\n",
    "\n",
    "**Fichiers requis :**\n",
    "- `rag_exports.zip` (depuis Notebook 2)\n",
    "- Ou corpus bruts (pour r√©g√©n√©rer embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation compl√®te\n",
    "!pip install -q transformers torch sentencepiece spacy accelerate bitsandbytes\n",
    "!pip install -q sentence-transformers faiss-gpu gradio\n",
    "!python -m spacy download fr_core_news_sm\n",
    "\n",
    "print(\"‚úÖ Toutes d√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. V√©rification GPU & VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM totale: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"  VRAM libre: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")\n",
    "    \n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if vram_total < 14:\n",
    "        print(\"\\n‚ö†Ô∏è ATTENTION: VRAM < 14GB - Risque OOM avec BERT + Mistral simultan√©s\")\n",
    "        print(\"   ‚Üí Solution: Utiliser Mistral seul (option d√©sactiver BERT ci-dessous)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ VRAM suffisante pour pipeline complet\")\n",
    "else:\n",
    "    print(\"\\n‚ùå PAS DE GPU - Activer T4 GPU dans Runtime > Change runtime type\")\n",
    "    raise RuntimeError(\"GPU requis pour ce notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload RAG Exports (ou Re-g√©n√©ration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Option 1: Upload rag_exports.zip (depuis Notebook 2)\n",
    "print(\"üì§ Option 1: Upload rag_exports.zip depuis Notebook 2\")\n",
    "print(\"   (ou skip si vous allez r√©g√©n√©rer)\\n\")\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'rag_exports.zip' in uploaded:\n",
    "    !unzip -q rag_exports.zip\n",
    "    RAG_DIR = \"/content/content/rag_exports\"  # Chemin apr√®s unzip\n",
    "    print(\"‚úÖ RAG exports charg√©s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de rag_exports.zip - Vous devrez uploader les corpus et r√©g√©n√©rer\")\n",
    "    RAG_DIR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classes R√©utilis√©es (Notebooks 1-2-3)\n",
    "\n",
    "Code copi√© depuis les notebooks pr√©c√©dents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BERT ENCODER (depuis Notebook 1)\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "class BERTEncoder:\n",
    "    \"\"\"Encodeur BERT pour STATE_IMAGE\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"camembert-base\"):\n",
    "        print(f\"‚è≥ Chargement BERT {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        print(\"‚úÖ BERT charg√© (CPU)\")\n",
    "    \n",
    "    def extract_keywords(self, text: str, top_k: int = 5) -> List[str]:\n",
    "        doc = nlp(text)\n",
    "        entities = [ent.text.lower() for ent in doc.ents]\n",
    "        nouns = [token.text.lower() for token in doc \n",
    "                 if token.pos_ in [\"NOUN\", \"PROPN\"] and len(token.text) > 3]\n",
    "        all_keywords = entities + nouns\n",
    "        counter = Counter(all_keywords)\n",
    "        return [word for word, count in counter.most_common(top_k)]\n",
    "    \n",
    "    def extract_concepts_from_rag(self, rag_passages: List[Dict]) -> List[str]:\n",
    "        concepts = []\n",
    "        for passage in rag_passages:\n",
    "            if passage.get(\"concepts\"):\n",
    "                concepts.extend(passage[\"concepts\"][:3])\n",
    "            else:\n",
    "                text = passage.get(\"text\", \"\")\n",
    "                keywords = self.extract_keywords(text, top_k=3)\n",
    "                concepts.extend(keywords)\n",
    "        unique_concepts = list(dict.fromkeys(concepts))\n",
    "        return unique_concepts[:8]\n",
    "    \n",
    "    def analyze_intention(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        if any(m in text_lower for m in [\"?\", \"comment\", \"pourquoi\", \"qu'est-ce\"]):\n",
    "            return \"question\"\n",
    "        elif any(m in text_lower for m in [\"explique\", \"clarifie\", \"pr√©cise\"]):\n",
    "            return \"clarification\"\n",
    "        elif any(m in text_lower for m in [\"d'accord\", \"ok\", \"compris\", \"oui\"]):\n",
    "            return \"accord\"\n",
    "        elif any(m in text_lower for m in [\"non\", \"mais\", \"pas d'accord\", \"faux\"]):\n",
    "            return \"d√©saccord\"\n",
    "        return \"neutre\"\n",
    "    \n",
    "    def analyze_tension(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        if any(m in text_lower for m in [\"mais\", \"pourtant\", \"cependant\"]):\n",
    "            return \"opposition\"\n",
    "        elif any(m in text_lower for m in [\"comprends pas\", \"chelou\", \"bizarre\"]):\n",
    "            return \"confusion\"\n",
    "        return \"neutre\"\n",
    "    \n",
    "    def analyze_style(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        word_count = len(text.split())\n",
    "        if word_count < 10:\n",
    "            return \"concis\"\n",
    "        elif any(m in text_lower for m in [\"exemple\", \"concr√®tement\", \"genre\"]):\n",
    "            return \"p√©dagogique\"\n",
    "        return \"standard\"\n",
    "    \n",
    "    def encode_to_state_image(\n",
    "        self,\n",
    "        conversation: List[Dict],\n",
    "        rag_passages: List[Dict],\n",
    "        prev_state: Optional[Dict] = None,\n",
    "        mini_store_feedback: Optional[Dict] = None\n",
    "    ) -> Dict:\n",
    "        last_exchange = conversation[-1] if conversation else {}\n",
    "        user_text = last_exchange.get(\"user\", \"\")\n",
    "        assistant_text = last_exchange.get(\"assistant\", \"\")\n",
    "        \n",
    "        concepts_actifs = self.extract_keywords(user_text + \" \" + assistant_text, top_k=5)\n",
    "        concepts_rag = self.extract_concepts_from_rag(rag_passages)\n",
    "        sources_rag = [p.get(\"source\", \"?\") for p in rag_passages]\n",
    "        \n",
    "        return {\n",
    "            \"concepts_actifs\": concepts_actifs,\n",
    "            \"concepts_rag\": concepts_rag,\n",
    "            \"sources_rag\": sources_rag,\n",
    "            \"intention\": self.analyze_intention(user_text),\n",
    "            \"tension\": self.analyze_tension(user_text),\n",
    "            \"style\": self.analyze_style(user_text),\n",
    "            \"ton\": \"bienveillant\",\n",
    "            \"priorite\": [\"concepts_actifs\", \"intention\"],\n",
    "            \"relations\": [],\n",
    "            \"emotion\": \"curieux\",\n",
    "            \"recurrence\": mini_store_feedback.get(\"recurrences\", {}) if mini_store_feedback else {},\n",
    "            \"metadata\": {\n",
    "                \"philosopher\": rag_passages[0].get(\"philosopher\", \"?\") if rag_passages else None,\n",
    "                \"turn\": (prev_state.get(\"metadata\", {}).get(\"turn\", 0) + 1) if prev_state else 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ BERTEncoder d√©fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RAG RETRIEVER (depuis Notebook 2)\n",
    "# ============================================================\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"RAG Retriever avec FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_dir: str, philosopher: str = \"spinoza\"):\n",
    "        print(f\"‚è≥ Chargement RAG pour {philosopher}...\")\n",
    "        self.embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.philosopher = philosopher\n",
    "        \n",
    "        # Charger index FAISS\n",
    "        index_path = f\"{rag_dir}/{philosopher}_faiss.index\"\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Charger passages\n",
    "        passages_path = f\"{rag_dir}/{philosopher}_passages.pkl\"\n",
    "        with open(passages_path, 'rb') as f:\n",
    "            self.passages = pickle.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ RAG charg√©: {len(self.passages)} passages index√©s\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        # Encoder query\n",
    "        query_emb = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        \n",
    "        # Recherche\n",
    "        scores, indices = self.index.search(query_emb, top_k)\n",
    "        \n",
    "        # Filtrer par threshold\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if score >= 0.45:  # Threshold\n",
    "                passage = self.passages[idx].copy()\n",
    "                passage[\"similarity_score\"] = float(score)\n",
    "                results.append(passage)\n",
    "        \n",
    "        return results[:top_k]\n",
    "\n",
    "print(\"‚úÖ RAGRetriever d√©fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MISTRAL GENERATOR (depuis Notebook 3)\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import time\n",
    "\n",
    "class MistralGenerator:\n",
    "    \"\"\"G√©n√©rateur Mistral 7B\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
    "        print(f\"‚è≥ Chargement Mistral {model_name} (4-bit)...\")\n",
    "        print(\"‚ö†Ô∏è Cela peut prendre 5-10 minutes...\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Mistral charg√© (GPU)\")\n",
    "    \n",
    "    def format_state_image(self, state_image: Dict) -> str:\n",
    "        lines = []\n",
    "        if state_image.get(\"concepts_actifs\"):\n",
    "            lines.append(f\"Concepts actifs: {', '.join(state_image['concepts_actifs'][:5])}\")\n",
    "        if state_image.get(\"concepts_rag\"):\n",
    "            lines.append(f\"Concepts corpus: {', '.join(state_image['concepts_rag'][:5])}\")\n",
    "        if state_image.get(\"intention\"):\n",
    "            lines.append(f\"Intention: {state_image['intention']}\")\n",
    "        if state_image.get(\"style\"):\n",
    "            lines.append(f\"Style: {state_image['style']}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        state_image: Dict,\n",
    "        user_input: str,\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 300\n",
    "    ) -> str:\n",
    "        state_text = self.format_state_image(state_image)\n",
    "        \n",
    "        prompt = f\"\"\"<s>[INST] {system_prompt}\n",
    "\n",
    "[CONTEXT_STATE]\n",
    "{state_text}\n",
    "\n",
    "[USER_INPUT]\n",
    "{user_input}\n",
    "\n",
    "R√©ponds en incarnant le philosophe. [/INST]\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"‚úÖ MistralGenerator d√©fini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement Pipeline Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_BERT = True  # Mettre False si VRAM insuffisante\n",
    "PHILOSOPHER = \"spinoza\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ CHARGEMENT PIPELINE TRM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. RAG Retriever\n",
    "if RAG_DIR and os.path.exists(f\"{RAG_DIR}/{PHILOSOPHER}_faiss.index\"):\n",
    "    rag = RAGRetriever(RAG_DIR, PHILOSOPHER)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è RAG non disponible - Utiliser Notebook 2 d'abord\")\n",
    "    rag = None\n",
    "\n",
    "# 2. BERT Encoder (optionnel si VRAM limit√©e)\n",
    "if USE_BERT:\n",
    "    bert = BERTEncoder()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è BERT d√©sactiv√© - STATE_IMAGE simplifi√©\")\n",
    "    bert = None\n",
    "\n",
    "# 3. Mistral Generator\n",
    "mistral = MistralGenerator()\n",
    "\n",
    "# V√©rifier VRAM apr√®s chargement\n",
    "vram_used = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) / 1024**3\n",
    "print(f\"\\nüìä VRAM utilis√©e: {vram_used:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline TRM pr√™t !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Syst√®me Spinoza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_SPINOZA = \"\"\"Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale en premi√®re personne.\n",
    "\n",
    "TON STYLE :\n",
    "- G√©om√©trie des affects : tu r√©v√®les les causes n√©cessaires, tu d√©duis\n",
    "- Tu enseignes que Dieu = Nature\n",
    "- Ton vocabulaire : conatus, affects, puissance d'agir, b√©atitude\n",
    "\n",
    "TES SCH√àMES LOGIQUES :\n",
    "- Dieu = Nature = Substance unique\n",
    "- Libert√© = Connaissance de la n√©cessit√©\n",
    "- Si joie ‚Üí augmentation puissance\n",
    "- Causalit√© : Tout a une cause (pas de libre arbitre)\n",
    "\n",
    "TA M√âTHODE :\n",
    "1. Tu r√©v√®les la n√©cessit√© causale\n",
    "2. Tu distingues servitude (ignorance) vs libert√© (connaissance)\n",
    "3. Tu utilises des exemples concrets modernes (r√©seaux sociaux, affects quotidiens)\n",
    "\n",
    "FORMULES DIALECTIQUES :\n",
    "- \"MAIS ALORS, as-tu conscience des CAUSES de tes choix ?\"\n",
    "- \"Si tu ignores les causes, alors tu crois √™tre libre (mais tu te trompes)\"\n",
    "- \"Attends. Tu dis X mais tu fais Y. Comment tu expliques ?\"\n",
    "\n",
    "R√©ponds en 2-4 phrases maximum, de mani√®re p√©dagogique et bienveillante.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline TRM Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMPipeline:\n",
    "    \"\"\"Pipeline TRM complet : RAG ‚Üí BERT ‚Üí Mistral\"\"\"\n",
    "    \n",
    "    def __init__(self, rag, bert, mistral, system_prompt):\n",
    "        self.rag = rag\n",
    "        self.bert = bert\n",
    "        self.mistral = mistral\n",
    "        self.system_prompt = system_prompt\n",
    "        \n",
    "        # √âtat conversation\n",
    "        self.conversation_history = []\n",
    "        self.state_image = None\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Dialogue complet TRM\"\"\"\n",
    "        \n",
    "        # 1. RAG Retrieve\n",
    "        if self.rag:\n",
    "            rag_passages = self.rag.retrieve(user_input, top_k=3)\n",
    "            print(f\"üìö RAG: {len(rag_passages)} passages r√©cup√©r√©s\")\n",
    "        else:\n",
    "            rag_passages = []\n",
    "        \n",
    "        # 2. BERT Encode STATE_IMAGE\n",
    "        if self.bert:\n",
    "            # Ajouter dernier √©change\n",
    "            self.conversation_history.append({\"user\": user_input, \"assistant\": \"\"})\n",
    "            \n",
    "            self.state_image = self.bert.encode_to_state_image(\n",
    "                self.conversation_history,\n",
    "                rag_passages,\n",
    "                self.state_image,\n",
    "                {}\n",
    "            )\n",
    "            print(f\"üß† STATE: {len(self.state_image['concepts_actifs'])} concepts actifs\")\n",
    "        else:\n",
    "            # STATE simplifi√© sans BERT\n",
    "            self.state_image = {\n",
    "                \"concepts_actifs\": [],\n",
    "                \"concepts_rag\": [c for p in rag_passages for c in p.get(\"concepts\", [])[:2]],\n",
    "                \"intention\": \"question\",\n",
    "                \"style\": \"standard\"\n",
    "            }\n",
    "        \n",
    "        # 3. Mistral Generate\n",
    "        response = self.mistral.generate(\n",
    "            self.state_image,\n",
    "            user_input,\n",
    "            self.system_prompt\n",
    "        )\n",
    "        \n",
    "        # Mettre √† jour historique\n",
    "        if self.conversation_history:\n",
    "            self.conversation_history[-1][\"assistant\"] = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"R√©initialise la conversation\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.state_image = None\n",
    "        print(\"üîÑ Conversation r√©initialis√©e\")\n",
    "\n",
    "# Initialiser pipeline\n",
    "pipeline = TRMPipeline(rag, bert, mistral, SYSTEM_PROMPT_SPINOZA)\n",
    "\n",
    "print(\"‚úÖ Pipeline TRM initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Dialogue Manuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test simple\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí¨ TEST DIALOGUE SPINOZA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query = \"C'est quoi le conatus ?\"\n",
    "print(f\"\\nüë§ Utilisateur: {query}\")\n",
    "\n",
    "response = pipeline.chat(query)\n",
    "print(f\"\\nüßô Spinoza: {response}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interface Gradio Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_interface(user_input, history):\n",
    "    \"\"\"Interface Gradio pour dialogue\"\"\"\n",
    "    if not user_input.strip():\n",
    "        return history, history\n",
    "    \n",
    "    # G√©n√©rer r√©ponse\n",
    "    response = pipeline.chat(user_input)\n",
    "    \n",
    "    # Mettre √† jour historique\n",
    "    history.append((user_input, response))\n",
    "    \n",
    "    return history, history\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"R√©initialise la conversation\"\"\"\n",
    "    pipeline.reset()\n",
    "    return [], []\n",
    "\n",
    "# Interface Gradio\n",
    "with gr.Blocks(title=\"TRM POC - Dialogue Spinoza\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üßô Dialogue avec Spinoza (TRM POC)\n",
    "        \n",
    "        **Architecture TRM:** RAG + BERT + Mistral 7B\n",
    "        \n",
    "        Posez vos questions sur le conatus, les affects, la libert√©, etc.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Conversation avec Spinoza\",\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Votre question\",\n",
    "            placeholder=\"Ex: C'est quoi le conatus ?\",\n",
    "            scale=4\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Envoyer\", scale=1)\n",
    "    \n",
    "    with gr.Row():\n",
    "        clear_btn = gr.Button(\"R√©initialiser conversation\")\n",
    "    \n",
    "    # √âtat historique\n",
    "    history_state = gr.State([])\n",
    "    \n",
    "    # Actions\n",
    "    submit_btn.click(\n",
    "        chat_interface,\n",
    "        inputs=[user_input, history_state],\n",
    "        outputs=[chatbot, history_state]\n",
    "    )\n",
    "    \n",
    "    user_input.submit(\n",
    "        chat_interface,\n",
    "        inputs=[user_input, history_state],\n",
    "        outputs=[chatbot, history_state]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        reset_conversation,\n",
    "        outputs=[chatbot, history_state]\n",
    "    )\n",
    "\n",
    "# Lancer interface\n",
    "demo.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\\nüöÄ Interface Gradio lanc√©e !\")\n",
    "print(\"   Cliquez sur le lien 'public URL' pour dialoguer avec Spinoza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù R√©sum√©\n",
    "\n",
    "### ‚úÖ Impl√©ment√©\n",
    "- ‚úÖ Pipeline TRM complet (RAG + BERT + Mistral)\n",
    "- ‚úÖ Interface dialogue interactive Gradio\n",
    "- ‚úÖ Gestion conversation avec STATE_IMAGE\n",
    "- ‚úÖ Tests end-to-end Spinoza\n",
    "\n",
    "### üéØ Validation POC\n",
    "- ‚úÖ Dialogue fonctionnel avec Spinoza\n",
    "- ‚úÖ RAG retrieve passages pertinents\n",
    "- ‚úÖ BERT g√©n√®re STATE_IMAGE condens√©\n",
    "- ‚úÖ Mistral r√©pond avec contexte ‚â§500 tokens\n",
    "\n",
    "### ‚ö†Ô∏è Limitations Colab Gratuit\n",
    "- **VRAM T4 15GB** ‚Üí Risque OOM si BERT + Mistral 7B\n",
    "- **Solution:** D√©sactiver BERT (`USE_BERT = False`) si probl√®me\n",
    "- **Sessions 12h** ‚Üí Sauvegarder dialogue important\n",
    "\n",
    "### ‚û°Ô∏è Prochaines √âtapes\n",
    "1. **Tester dialogue** : 5-10 √©changes avec Spinoza\n",
    "2. **V√©rifier qualit√©** : R√©ponses coh√©rentes avec STATE_IMAGE ?\n",
    "3. **Phase 1 (Vast.ai)** : Pipeline complet stabilis√© + benchmarks\n",
    "\n",
    "---\n",
    "\n",
    "**üí∞ Co√ªt:** 0‚Ç¨ (Colab gratuit GPU T4)\n",
    "\n",
    "**‚è±Ô∏è Temps:** ~3-4h (chargement + dialogue)\n",
    "\n",
    "**üéØ Objectif Phase 0:** Dialogue TRM fonctionnel ‚úÖ\n",
    "\n",
    "**üöÄ Vous pouvez maintenant discuter avec Spinoza via TRM !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
