{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† TRM POC - Notebook 1: BERT Encoder + STATE_IMAGE\n",
    "\n",
    "**Objectif:** D√©velopper et tester le BERT Encoder pour g√©n√©rer le STATE_IMAGE structur√©\n",
    "\n",
    "**Runtime:** CPU Colab gratuit (BERT ne n√©cessite pas GPU)\n",
    "\n",
    "**Dur√©e estim√©e:** 2-3h d√©veloppement + tests\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 0 - POC TRM (0‚Ç¨)\n",
    "\n",
    "Ce notebook impl√©mente:\n",
    "1. Chargement BERT-base (CPU)\n",
    "2. Extraction concepts conversationnels\n",
    "3. Extraction concepts RAG (depuis passages)\n",
    "4. Analyse multi-axes (intention, tension, style, etc.)\n",
    "5. G√©n√©ration STATE_IMAGE JSON structur√©\n",
    "\n",
    "**Note:** Pas d'inf√©rence compl√®te ici - juste la logique BERT isol√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies n√©cessaires\n",
    "!pip install -q transformers torch sentencepiece spacy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Charger mod√®le spaCy pour extraction entit√©s\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Configuration\n",
    "BERT_MODEL = \"camembert-base\"  # Mod√®le fran√ßais optimis√©\n",
    "# Alternative: \"bert-base-multilingual-cased\" si CamemBERT trop lourd\n",
    "\n",
    "print(\"‚úÖ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classe BERTEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder:\n",
    "    \"\"\"\n",
    "    Encodeur BERT pour g√©n√©rer STATE_IMAGE condens√©.\n",
    "    Optimis√© pour CPU (Colab gratuit).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = BERT_MODEL):\n",
    "        print(f\"‚è≥ Chargement {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()  # Mode √©valuation (pas training)\n",
    "        print(\"‚úÖ BERT charg√© (CPU)\")\n",
    "    \n",
    "    def extract_keywords(self, text: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extrait mots-cl√©s via spaCy (NER + fr√©quence).\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Entit√©s nomm√©es\n",
    "        entities = [ent.text.lower() for ent in doc.ents]\n",
    "        \n",
    "        # Noms propres et concepts (NOUN, PROPN)\n",
    "        nouns = [token.text.lower() for token in doc \n",
    "                 if token.pos_ in [\"NOUN\", \"PROPN\"] \n",
    "                 and len(token.text) > 3]\n",
    "        \n",
    "        # Fr√©quence combin√©e\n",
    "        all_keywords = entities + nouns\n",
    "        counter = Counter(all_keywords)\n",
    "        \n",
    "        return [word for word, count in counter.most_common(top_k)]\n",
    "    \n",
    "    def extract_concepts_from_rag(self, rag_passages: List[Dict]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extrait concepts condens√©s des passages RAG (PAS texte brut).\n",
    "        \n",
    "        Args:\n",
    "            rag_passages: [{'text': '...', 'concepts': [...], 'source': '...'}]\n",
    "        \n",
    "        Returns:\n",
    "            Liste de concepts condens√©s (max 8)\n",
    "        \"\"\"\n",
    "        concepts = []\n",
    "        \n",
    "        for passage in rag_passages:\n",
    "            # M√©thode 1: Utiliser concepts pr√©-annot√©s si disponibles\n",
    "            if passage.get(\"concepts\"):\n",
    "                concepts.extend(passage[\"concepts\"][:3])  # Top 3 par passage\n",
    "            else:\n",
    "                # M√©thode 2: Extraction keywords du texte\n",
    "                text = passage.get(\"text\", \"\")\n",
    "                keywords = self.extract_keywords(text, top_k=3)\n",
    "                concepts.extend(keywords)\n",
    "        \n",
    "        # D√©dupliquer et limiter √† 8 concepts\n",
    "        unique_concepts = list(dict.fromkeys(concepts))  # Pr√©serve l'ordre\n",
    "        return unique_concepts[:8]\n",
    "    \n",
    "    def analyze_intention(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        D√©tecte l'intention de l'utilisateur.\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(marker in text_lower for marker in [\"?\", \"comment\", \"pourquoi\", \"qu'est-ce\"]):\n",
    "            return \"question\"\n",
    "        elif any(marker in text_lower for marker in [\"explique\", \"clarifie\", \"pr√©cise\"]):\n",
    "            return \"clarification\"\n",
    "        elif any(marker in text_lower for marker in [\"d'accord\", \"ok\", \"compris\", \"oui\"]):\n",
    "            return \"accord\"\n",
    "        elif any(marker in text_lower for marker in [\"non\", \"mais\", \"pas d'accord\", \"faux\"]):\n",
    "            return \"d√©saccord\"\n",
    "        else:\n",
    "            return \"neutre\"\n",
    "    \n",
    "    def analyze_tension(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        D√©tecte la tension/r√©sistance dans le message.\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(marker in text_lower for marker in [\"mais\", \"pourtant\", \"cependant\", \"toutefois\"]):\n",
    "            return \"opposition\"\n",
    "        elif any(marker in text_lower for marker in [\"comprends pas\", \"chelou\", \"bizarre\"]):\n",
    "            return \"confusion\"\n",
    "        else:\n",
    "            return \"neutre\"\n",
    "    \n",
    "    def analyze_style(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        D√©tecte le style conversationnel souhait√©.\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        if word_count < 10:\n",
    "            return \"concis\"\n",
    "        elif any(marker in text_lower for marker in [\"üòÇ\", \"lol\", \"mdr\"]):\n",
    "            return \"humoristique\"\n",
    "        elif any(marker in text_lower for marker in [\"exemple\", \"concr√®tement\", \"genre\"]):\n",
    "            return \"p√©dagogique\"\n",
    "        else:\n",
    "            return \"standard\"\n",
    "    \n",
    "    def encode_to_state_image(\n",
    "        self,\n",
    "        conversation: List[Dict],\n",
    "        rag_passages: List[Dict],\n",
    "        prev_state: Optional[Dict] = None,\n",
    "        mini_store_feedback: Optional[Dict] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        G√©n√®re STATE_IMAGE structur√© complet.\n",
    "        \n",
    "        Args:\n",
    "            conversation: [{'user': '...', 'assistant': '...'}]\n",
    "            rag_passages: [{'text': '...', 'concepts': [...]}]\n",
    "            prev_state: STATE_IMAGE pr√©c√©dent (optionnel)\n",
    "            mini_store_feedback: Feedback Mini-store (optionnel)\n",
    "        \n",
    "        Returns:\n",
    "            STATE_IMAGE JSON complet\n",
    "        \"\"\"\n",
    "        # Extraire dernier √©change\n",
    "        last_exchange = conversation[-1] if conversation else {}\n",
    "        user_text = last_exchange.get(\"user\", \"\")\n",
    "        assistant_text = last_exchange.get(\"assistant\", \"\")\n",
    "        \n",
    "        # Extraction concepts conversationnels\n",
    "        concepts_actifs = self.extract_keywords(user_text + \" \" + assistant_text, top_k=5)\n",
    "        \n",
    "        # Extraction concepts RAG (condens√©s)\n",
    "        concepts_rag = self.extract_concepts_from_rag(rag_passages)\n",
    "        sources_rag = [p.get(\"source\", \"?\") for p in rag_passages]\n",
    "        \n",
    "        # Analyse multi-axes\n",
    "        intention = self.analyze_intention(user_text)\n",
    "        tension = self.analyze_tension(user_text)\n",
    "        style = self.analyze_style(user_text)\n",
    "        \n",
    "        # Construire STATE_IMAGE\n",
    "        state_image = {\n",
    "            \"concepts_actifs\": concepts_actifs,\n",
    "            \"concepts_rag\": concepts_rag,\n",
    "            \"sources_rag\": sources_rag,\n",
    "            \"intention\": intention,\n",
    "            \"tension\": tension,\n",
    "            \"style\": style,\n",
    "            \"ton\": \"bienveillant\",  # Par d√©faut\n",
    "            \"priorite\": [\"concepts_actifs\", \"intention\"],\n",
    "            \"relations\": [],  # √Ä impl√©menter si besoin\n",
    "            \"emotion\": \"curieux\",  # Heuristique simple\n",
    "            \"recurrence\": mini_store_feedback.get(\"recurrences\", {}) if mini_store_feedback else {},\n",
    "            \"metadata\": {\n",
    "                \"philosopher\": rag_passages[0].get(\"philosopher\", \"?\") if rag_passages else None,\n",
    "                \"turn\": (prev_state.get(\"metadata\", {}).get(\"turn\", 0) + 1) if prev_state else 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return state_image\n",
    "\n",
    "print(\"‚úÖ Classe BERTEncoder d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tests Unitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser BERT Encoder\n",
    "encoder = BERTEncoder()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ TESTS UNITAIRES\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Extraction Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Le conatus est l'effort par lequel chaque chose s'efforce de pers√©v√©rer dans son √™tre. C'est la puissance d'agir selon Spinoza.\"\n",
    "\n",
    "keywords = encoder.extract_keywords(test_text, top_k=5)\n",
    "print(\"\\nüìå Test 1: Extraction Keywords\")\n",
    "print(f\"Texte: {test_text[:80]}...\")\n",
    "print(f\"Keywords extraits: {keywords}\")\n",
    "assert len(keywords) > 0, \"√âchec: Pas de keywords extraits\"\n",
    "print(\"‚úÖ Test 1 OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Extraction Concepts RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_passages = [\n",
    "    {\n",
    "        \"text\": \"Le conatus est l'effort pour pers√©v√©rer...\",\n",
    "        \"concepts\": [\"conatus\", \"effort\", \"pers√©v√©rer\"],\n",
    "        \"source\": \"Spinoza, √âthique III\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Les affects sont des modifications de la puissance d'agir...\",\n",
    "        \"concepts\": [\"affects\", \"puissance d'agir\"],\n",
    "        \"source\": \"Spinoza, √âthique III\"\n",
    "    }\n",
    "]\n",
    "\n",
    "concepts_rag = encoder.extract_concepts_from_rag(test_passages)\n",
    "print(\"\\nüìå Test 2: Extraction Concepts RAG\")\n",
    "print(f\"Passages: {len(test_passages)} fournis\")\n",
    "print(f\"Concepts extraits: {concepts_rag}\")\n",
    "assert len(concepts_rag) > 0, \"√âchec: Pas de concepts RAG extraits\"\n",
    "assert \"conatus\" in concepts_rag, \"√âchec: 'conatus' devrait √™tre extrait\"\n",
    "assert all(len(c) < 50 for c in concepts_rag), \"√âchec: Concepts trop longs (pas condens√©s)\"\n",
    "print(\"‚úÖ Test 2 OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Analyse Axes (Intention, Tension, Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    (\"Qu'est-ce que le conatus ?\", \"question\"),\n",
    "    (\"Je suis d'accord avec ta d√©monstration\", \"accord\"),\n",
    "    (\"Mais ce n'est pas logique !\", \"d√©saccord\"),\n",
    "    (\"Je comprends pas le rapport\", \"neutre\"),\n",
    "]\n",
    "\n",
    "print(\"\\nüìå Test 3: Analyse Intention\")\n",
    "for text, expected in test_cases:\n",
    "    intention = encoder.analyze_intention(text)\n",
    "    print(f\"  '{text}' ‚Üí {intention} (attendu: {expected})\")\n",
    "    assert intention == expected, f\"√âchec: attendu '{expected}', obtenu '{intention}'\"\n",
    "\n",
    "print(\"‚úÖ Test 3 OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: G√©n√©ration STATE_IMAGE Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conversation = [\n",
    "    {\n",
    "        \"user\": \"Peux-tu m'expliquer le conatus de Spinoza ?\",\n",
    "        \"assistant\": \"Le conatus est l'effort par lequel chaque chose pers√©v√®re dans son √™tre.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "state_image = encoder.encode_to_state_image(\n",
    "    conversation=test_conversation,\n",
    "    rag_passages=test_passages,\n",
    "    prev_state=None,\n",
    "    mini_store_feedback={}\n",
    ")\n",
    "\n",
    "print(\"\\nüìå Test 4: STATE_IMAGE Complet\")\n",
    "print(json.dumps(state_image, indent=2, ensure_ascii=False))\n",
    "\n",
    "# V√©rifications critiques\n",
    "assert \"concepts_actifs\" in state_image, \"√âchec: 'concepts_actifs' manquant\"\n",
    "assert \"concepts_rag\" in state_image, \"√âchec: 'concepts_rag' manquant\"\n",
    "assert len(state_image[\"concepts_rag\"]) > 0, \"√âchec: Concepts RAG vides\"\n",
    "assert state_image[\"intention\"] == \"question\", \"√âchec: Intention mal d√©tect√©e\"\n",
    "\n",
    "# V√©rifier qu'il n'y a PAS de texte brut RAG dans STATE_IMAGE\n",
    "state_str = json.dumps(state_image)\n",
    "assert \"l'effort pour pers√©v√©rer\" not in state_str, \"√âchec: Texte brut RAG pr√©sent dans STATE_IMAGE\"\n",
    "\n",
    "print(\"\\n‚úÖ Test 4 OK - STATE_IMAGE valide !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Taille STATE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimer taille en tokens (approximation)\n",
    "state_text = json.dumps(state_image, ensure_ascii=False)\n",
    "token_count = len(state_text.split())  # Approximation simple\n",
    "\n",
    "print(\"\\nüìä M√©triques STATE_IMAGE\")\n",
    "print(f\"  Taille JSON: {len(state_text)} caract√®res\")\n",
    "print(f\"  Tokens estim√©s: {token_count} (objectif: 150-250)\")\n",
    "print(f\"  Concepts actifs: {len(state_image['concepts_actifs'])}\")\n",
    "print(f\"  Concepts RAG: {len(state_image['concepts_rag'])}\")\n",
    "print(f\"  Sources RAG: {len(state_image['sources_rag'])}\")\n",
    "\n",
    "if token_count <= 250:\n",
    "    print(\"\\n‚úÖ Taille STATE_IMAGE optimale (‚â§250 tokens)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è STATE_IMAGE trop grand ({token_count} tokens) - √Ä optimiser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export pour R√©utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder classe BERTEncoder pour import dans Notebook 3 (int√©gration)\n",
    "import pickle\n",
    "\n",
    "# Sauvegarder instance (optionnel, si besoin)\n",
    "# with open('bert_encoder.pkl', 'wb') as f:\n",
    "#     pickle.dump(encoder, f)\n",
    "\n",
    "# Exporter STATE_IMAGE exemple\n",
    "with open('/content/state_image_example.json', 'w') as f:\n",
    "    json.dump(state_image, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nüíæ Fichiers export√©s:\")\n",
    "print(\"  - state_image_example.json\")\n",
    "print(\"\\n‚úÖ Notebook 1 termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù R√©sum√©\n",
    "\n",
    "### ‚úÖ Impl√©ment√©\n",
    "- ‚úÖ BERTEncoder charg√© (CPU - CamemBERT fran√ßais)\n",
    "- ‚úÖ Extraction keywords/concepts conversationnels\n",
    "- ‚úÖ Extraction concepts RAG condens√©s (pas texte brut)\n",
    "- ‚úÖ Analyse multi-axes (intention, tension, style)\n",
    "- ‚úÖ G√©n√©ration STATE_IMAGE JSON structur√©\n",
    "- ‚úÖ Tests unitaires valid√©s\n",
    "\n",
    "### üìä M√©triques Cibles POC\n",
    "- **Taille STATE_IMAGE:** 150-250 tokens ‚úÖ\n",
    "- **Concepts extraits:** 3-8 par tour ‚úÖ\n",
    "- **Pas de texte brut RAG:** ‚úÖ Valid√©\n",
    "- **Axes analys√©s:** 9/9 (complet) ‚úÖ\n",
    "\n",
    "### ‚û°Ô∏è Prochaines √âtapes\n",
    "1. **Notebook 2:** G√©n√©ration embeddings RAG (sentence-transformers)\n",
    "2. **Notebook 3:** Tests Mistral 7B (GPU Colab T4)\n",
    "3. **Int√©gration:** Pipeline complet BERT ‚Üí Mistral\n",
    "\n",
    "---\n",
    "\n",
    "**üí∞ Co√ªt:** 0‚Ç¨ (Colab gratuit CPU)\n",
    "\n",
    "**‚è±Ô∏è Temps:** ~2-3h d√©veloppement + tests\n",
    "\n",
    "**üéØ Objectif Phase 0:** D√©velopper composants sans infrastructure payante ‚úÖ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
