{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç TRM POC - Notebook 2: RAG Embeddings Generation\n",
    "\n",
    "**Objectif:** G√©n√©rer les embeddings s√©mantiques des corpus (Spinoza/Bergson/Kant) avec sentence-transformers\n",
    "\n",
    "**Runtime:** GPU Colab gratuit (T4 - 12h max)\n",
    "\n",
    "**Dur√©e estim√©e:** 1-2h (g√©n√©ration + indexation)\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 0 - POC TRM (0‚Ç¨)\n",
    "\n",
    "Ce notebook impl√©mente:\n",
    "1. Upload corpus philosophiques (Spinoza/Bergson/Kant)\n",
    "2. D√©coupage en passages (sections Markdown)\n",
    "3. G√©n√©ration embeddings (sentence-transformers)\n",
    "4. Indexation FAISS pour retrieval rapide\n",
    "5. Export index pour r√©utilisation (Notebook 3 + Vast.ai)\n",
    "\n",
    "**Note:** GPU T4 gratuit suffisant pour embeddings (mod√®le l√©ger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies n√©cessaires\n",
    "!pip install -q sentence-transformers faiss-gpu\n",
    "\n",
    "print(\"‚úÖ D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Corpus Fichiers\n",
    "\n",
    "**Action manuelle:** Uploader les fichiers suivants depuis `bergsonAndFriends/data/RAG/`:\n",
    "- `Corpus Spinoza Dialogique 18k - √âthique II-IV.md`\n",
    "- `Glossaire Conversationnel Spinoza - 12 Concepts.md`\n",
    "- `corpus_bergson_27k_dialogique.md`\n",
    "- `glossaire_bergson_conversationnel.md`\n",
    "- `corpus_kant_20k.txt.md`\n",
    "- `glossaire_kant_conversationnel.md`\n",
    "\n",
    "Ou cloner le repo directement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload manuel via interface Colab\n",
    "from google.colab import files\n",
    "print(\"üì§ Upload fichiers corpus (6 fichiers .md attendus)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Option 2: Clone repo (si public)\n",
    "# !git clone https://github.com/YOUR_USERNAME/bergsonAndFriends.git\n",
    "# CORPUS_DIR = \"bergsonAndFriends/data/RAG/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # L√©ger, rapide, multilingue\n",
    "# Alternative: \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "CORPUS_FILES = {\n",
    "    \"spinoza\": {\n",
    "        \"corpus\": \"Corpus Spinoza Dialogique 18k - √âthique II-IV.md\",\n",
    "        \"glossaire\": \"Glossaire Conversationnel Spinoza - 12 Concepts.md\"\n",
    "    },\n",
    "    \"bergson\": {\n",
    "        \"corpus\": \"corpus_bergson_27k_dialogique.md\",\n",
    "        \"glossaire\": \"glossaire_bergson_conversationnel.md\"\n",
    "    },\n",
    "    \"kant\": {\n",
    "        \"corpus\": \"corpus_kant_20k.txt.md\",\n",
    "        \"glossaire\": \"glossaire_kant_conversationnel.md\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement Mod√®le Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚è≥ Chargement {EMBEDDING_MODEL}...\")\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"‚úÖ Mod√®le charg√© - Dimension: {embedder.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parsing Corpus en Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_corpus_to_passages(text: str, philosopher: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    D√©coupe corpus markdown en passages (bas√© sur headers ##).\n",
    "    \n",
    "    Returns:\n",
    "        Liste de passages avec metadata\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    lines = text.split('\\n')\n",
    "    current_section = {\"title\": \"\", \"content\": \"\"}\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('##'):\n",
    "            # Sauvegarder section pr√©c√©dente\n",
    "            if current_section[\"content\"].strip():\n",
    "                passages.append({\n",
    "                    \"text\": f\"{current_section['title']}\\n{current_section['content']}\",\n",
    "                    \"title\": current_section[\"title\"],\n",
    "                    \"philosopher\": philosopher,\n",
    "                    \"type\": \"corpus\"\n",
    "                })\n",
    "            # Nouvelle section\n",
    "            current_section = {\n",
    "                \"title\": re.sub(r'^#+\\s*', '', line),\n",
    "                \"content\": \"\"\n",
    "            }\n",
    "        else:\n",
    "            current_section[\"content\"] += line + '\\n'\n",
    "    \n",
    "    # Derni√®re section\n",
    "    if current_section[\"content\"].strip():\n",
    "        passages.append({\n",
    "            \"text\": f\"{current_section['title']}\\n{current_section['content']}\",\n",
    "            \"title\": current_section[\"title\"],\n",
    "            \"philosopher\": philosopher,\n",
    "            \"type\": \"corpus\"\n",
    "        })\n",
    "    \n",
    "    return passages\n",
    "\n",
    "print(\"‚úÖ Fonction parsing d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. G√©n√©ration Embeddings Par Philosophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage passages + embeddings par philosophe\n",
    "all_data = {}\n",
    "\n",
    "for philosopher, files in CORPUS_FILES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìö Traitement: {philosopher.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    passages = []\n",
    "    \n",
    "    # Charger corpus\n",
    "    corpus_file = files[\"corpus\"]\n",
    "    if Path(corpus_file).exists():\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            corpus_text = f.read()\n",
    "        corpus_passages = parse_corpus_to_passages(corpus_text, philosopher)\n",
    "        passages.extend(corpus_passages)\n",
    "        print(f\"  ‚úÖ Corpus: {len(corpus_passages)} passages\")\n",
    "    \n",
    "    # Charger glossaire\n",
    "    glossaire_file = files[\"glossaire\"]\n",
    "    if Path(glossaire_file).exists():\n",
    "        with open(glossaire_file, 'r', encoding='utf-8') as f:\n",
    "            glossaire_text = f.read()\n",
    "        glossaire_passages = parse_corpus_to_passages(glossaire_text, philosopher)\n",
    "        for p in glossaire_passages:\n",
    "            p[\"type\"] = \"glossaire\"  # Marquer glossaire\n",
    "        passages.extend(glossaire_passages)\n",
    "        print(f\"  ‚úÖ Glossaire: {len(glossaire_passages)} passages\")\n",
    "    \n",
    "    print(f\"  üìä Total passages: {len(passages)}\")\n",
    "    \n",
    "    # G√©n√©rer embeddings\n",
    "    print(f\"  ‚è≥ G√©n√©ration embeddings...\")\n",
    "    texts = [p[\"text\"] for p in passages]\n",
    "    embeddings = embedder.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    print(f\"  ‚úÖ Embeddings g√©n√©r√©s: {embeddings.shape}\")\n",
    "    \n",
    "    # Stocker\n",
    "    all_data[philosopher] = {\n",
    "        \"passages\": passages,\n",
    "        \"embeddings\": embeddings\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Tous les embeddings g√©n√©r√©s !\")\n",
    "for phil, data in all_data.items():\n",
    "    print(f\"  {phil}: {len(data['passages'])} passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Indexation FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er index FAISS par philosophe\n",
    "faiss_indexes = {}\n",
    "\n",
    "for philosopher, data in all_data.items():\n",
    "    print(f\"\\nüîç Indexation FAISS: {philosopher}\")\n",
    "    \n",
    "    embeddings = data[\"embeddings\"]\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Cr√©er index FAISS (IndexFlatIP = Inner Product, bon pour cosine similarity)\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    # Normaliser embeddings (pour cosine similarity)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Ajouter √† l'index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    faiss_indexes[philosopher] = index\n",
    "    print(f\"  ‚úÖ Index cr√©√©: {index.ntotal} vecteurs\")\n",
    "\n",
    "print(\"\\n‚úÖ Tous les index FAISS cr√©√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_passages(query: str, philosopher: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Retrieve passages pertinents via FAISS.\n",
    "    \"\"\"\n",
    "    # Encoder query\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_emb)\n",
    "    \n",
    "    # Recherche\n",
    "    index = faiss_indexes[philosopher]\n",
    "    scores, indices = index.search(query_emb, top_k)\n",
    "    \n",
    "    # R√©cup√©rer passages\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        passage = all_data[philosopher][\"passages\"][idx]\n",
    "        passage[\"similarity_score\"] = float(score)\n",
    "        results.append(passage)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "print(\"\\nüß™ TEST RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "test_query = \"Qu'est-ce que le conatus ?\"\n",
    "results = retrieve_passages(test_query, \"spinoza\", top_k=3)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nTop 3 passages (Spinoza):\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r['type']}] {r['title']} (score: {r['similarity_score']:.3f})\")\n",
    "    print(f\"   {r['text'][:150]}...\\n\")\n",
    "\n",
    "print(\"‚úÖ Retrieval fonctionnel !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export pour R√©utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er dossier export\n",
    "!mkdir -p /content/rag_exports\n",
    "\n",
    "# Sauvegarder index FAISS + passages par philosophe\n",
    "for philosopher in all_data.keys():\n",
    "    print(f\"\\nüíæ Export: {philosopher}\")\n",
    "    \n",
    "    # Index FAISS\n",
    "    faiss.write_index(faiss_indexes[philosopher], f\"/content/rag_exports/{philosopher}_faiss.index\")\n",
    "    print(f\"  ‚úÖ FAISS index saved\")\n",
    "    \n",
    "    # Passages (pickle)\n",
    "    with open(f\"/content/rag_exports/{philosopher}_passages.pkl\", 'wb') as f:\n",
    "        pickle.dump(all_data[philosopher][\"passages\"], f)\n",
    "    print(f\"  ‚úÖ Passages saved\")\n",
    "\n",
    "# Sauvegarder config\n",
    "config = {\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"dimension\": embedder.get_sentence_embedding_dimension(),\n",
    "    \"philosophers\": list(all_data.keys()),\n",
    "    \"total_passages\": {phil: len(data[\"passages\"]) for phil, data in all_data.items()}\n",
    "}\n",
    "\n",
    "with open('/content/rag_exports/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Tous les fichiers export√©s dans /content/rag_exports/\")\n",
    "print(\"\\nüìã Fichiers √† t√©l√©charger:\")\n",
    "!ls -lh /content/rag_exports/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. T√©l√©chargement ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er archive ZIP pour t√©l√©chargement\n",
    "!zip -r /content/rag_exports.zip /content/rag_exports/\n",
    "\n",
    "# T√©l√©charger via interface Colab\n",
    "from google.colab import files\n",
    "files.download('/content/rag_exports.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Archive t√©l√©charg√©e - √Ä uploader sur Vast.ai pour Phase 1 !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù R√©sum√©\n",
    "\n",
    "### ‚úÖ Impl√©ment√©\n",
    "- ‚úÖ Parsing 3 corpus (Spinoza/Bergson/Kant) en passages\n",
    "- ‚úÖ G√©n√©ration embeddings (sentence-transformers)\n",
    "- ‚úÖ Indexation FAISS par philosophe\n",
    "- ‚úÖ Retrieval s√©mantique fonctionnel\n",
    "- ‚úÖ Export index + passages pour r√©utilisation\n",
    "\n",
    "### üìä Statistiques\n",
    "- **Mod√®le:** all-MiniLM-L6-v2 (dimension: 384)\n",
    "- **Total passages:** ~XXX (√† compl√©ter apr√®s ex√©cution)\n",
    "- **Index FAISS:** 3 (Spinoza, Bergson, Kant)\n",
    "\n",
    "### üì¶ Fichiers Export√©s\n",
    "1. `spinoza_faiss.index` + `spinoza_passages.pkl`\n",
    "2. `bergson_faiss.index` + `bergson_passages.pkl`\n",
    "3. `kant_faiss.index` + `kant_passages.pkl`\n",
    "4. `config.json` (metadata)\n",
    "\n",
    "### ‚û°Ô∏è Prochaines √âtapes\n",
    "1. **Notebook 3:** Tests Mistral 7B (GPU Colab T4)\n",
    "2. **Phase 1 (Vast.ai):** Upload index RAG + int√©gration compl√®te\n",
    "\n",
    "---\n",
    "\n",
    "**üí∞ Co√ªt:** 0‚Ç¨ (Colab gratuit GPU T4)\n",
    "\n",
    "**‚è±Ô∏è Temps:** ~1-2h (d√©pend taille corpus)\n",
    "\n",
    "**üéØ Objectif Phase 0:** RAG s√©mantique pr√™t pour POC ‚úÖ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
