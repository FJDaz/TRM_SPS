{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ TRM POC - Notebook 3: Mistral 7B Testing\n",
    "\n",
    "**Objectif:** Tester Mistral 7B avec STATE_IMAGE et valider la g√©n√©ration avec contexte ‚â§500 tokens\n",
    "\n",
    "**Runtime:** GPU Colab gratuit (T4 - 15GB VRAM) - **ATTENTION: Sessions limit√©es 12h**\n",
    "\n",
    "**Dur√©e estim√©e:** 2-3h (chargement + tests + benchmarks)\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 0 - POC TRM (0‚Ç¨)\n",
    "\n",
    "Ce notebook impl√©mente:\n",
    "1. Chargement Mistral 7B (quantization 4-bit pour T4)\n",
    "2. Test g√©n√©ration avec STATE_IMAGE structur√©\n",
    "3. Validation taille contexte ‚â§500 tokens\n",
    "4. Mesure latence baseline\n",
    "5. Comparaison qualitative avec Qwen 14B (simul√©)\n",
    "\n",
    "**Note:** Colab T4 gratuit = 15GB VRAM ‚Üí n√©cessite quantization pour Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies n√©cessaires\n",
    "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
    "\n",
    "print(\"‚úÖ D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. V√©rification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM totale: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"  VRAM libre: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ATTENTION: Pas de GPU d√©tect√© - Activer GPU dans Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Alternative: \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Configuration quantization 4-bit (pour T4 15GB)\n",
    "QUANT_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚è≥ Chargement {MODEL_NAME} (4-bit quantization)...\")\n",
    "print(\"‚ö†Ô∏è Cela peut prendre 5-10 minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Charger mod√®le avec quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=QUANT_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√© en {load_time:.1f}s\")\n",
    "\n",
    "# V√©rifier VRAM utilis√©e\n",
    "vram_used = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) / 1024**3\n",
    "print(f\"üìä VRAM utilis√©e: {vram_used:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classe MistralGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralGenerator:\n",
    "    \"\"\"\n",
    "    G√©n√©rateur Mistral 7B pour TRM.\n",
    "    Lit STATE_IMAGE structur√© et g√©n√®re r√©ponse.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def format_state_image(self, state_image: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Formate STATE_IMAGE en texte structur√© pour prompt.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        if state_image.get(\"concepts_actifs\"):\n",
    "            lines.append(f\"Concepts actifs: {', '.join(state_image['concepts_actifs'][:5])}\")\n",
    "        \n",
    "        if state_image.get(\"concepts_rag\"):\n",
    "            lines.append(f\"Concepts pertinents (corpus): {', '.join(state_image['concepts_rag'][:5])}\")\n",
    "        \n",
    "        if state_image.get(\"intention\"):\n",
    "            lines.append(f\"Intention: {state_image['intention']}\")\n",
    "        \n",
    "        if state_image.get(\"tension\"):\n",
    "            lines.append(f\"Tension: {state_image['tension']}\")\n",
    "        \n",
    "        if state_image.get(\"style\"):\n",
    "            lines.append(f\"Style: {state_image['style']}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Compte tokens dans texte.\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        state_image: Dict,\n",
    "        user_input: str,\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 300\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        G√©n√®re r√©ponse avec STATE_IMAGE.\n",
    "        \n",
    "        Returns:\n",
    "            Dict avec 'response', 'context_tokens', 'generation_time'\n",
    "        \"\"\"\n",
    "        # Formater STATE_IMAGE\n",
    "        state_text = self.format_state_image(state_image)\n",
    "        \n",
    "        # Construire prompt\n",
    "        prompt = f\"\"\"<s>[INST] {system_prompt}\n",
    "\n",
    "[CONTEXT_STATE]\n",
    "{state_text}\n",
    "\n",
    "[USER_INPUT]\n",
    "{user_input}\n",
    "\n",
    "R√©ponds en incarnant le philosophe, en utilisant les concepts du STATE. [/INST]\"\"\"\n",
    "        \n",
    "        # Compter tokens contexte\n",
    "        context_tokens = self.count_tokens(prompt)\n",
    "        \n",
    "        # V√©rifier limite 500 tokens\n",
    "        if context_tokens > 500:\n",
    "            print(f\"‚ö†Ô∏è ATTENTION: Contexte trop grand ({context_tokens} tokens > 500)\")\n",
    "        \n",
    "        # G√©n√©rer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        # D√©coder r√©ponse\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"context_tokens\": context_tokens,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"tokens_per_second\": len(outputs[0]) / generation_time\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Classe MistralGenerator d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tests G√©n√©ration avec STATE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser g√©n√©rateur\n",
    "generator = MistralGenerator(model, tokenizer)\n",
    "\n",
    "# STATE_IMAGE exemple (depuis Notebook 1)\n",
    "test_state_image = {\n",
    "    \"concepts_actifs\": [\"conatus\", \"effort\", \"pers√©v√©rer\", \"puissance d'agir\"],\n",
    "    \"concepts_rag\": [\"conatus = puissance d'agir\", \"affects modifient conatus\"],\n",
    "    \"sources_rag\": [\"Spinoza, √âthique III, prop. 7\"],\n",
    "    \"intention\": \"question\",\n",
    "    \"tension\": \"neutre\",\n",
    "    \"style\": \"p√©dagogique\",\n",
    "    \"ton\": \"bienveillant\",\n",
    "    \"priorite\": [\"concepts_actifs\", \"intention\"],\n",
    "    \"metadata\": {\"philosopher\": \"spinoza\", \"turn\": 1}\n",
    "}\n",
    "\n",
    "# Prompt syst√®me Spinoza (simplifi√©)\n",
    "system_prompt = \"\"\"Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale.\n",
    "TON STYLE: G√©om√©trie des affects, d√©ductions logiques, exemples concrets modernes.\n",
    "TON VOCABULAIRE: conatus, affects, puissance d'agir, n√©cessit√© causale.\"\"\"\n",
    "\n",
    "# Test query\n",
    "user_input = \"Peux-tu m'expliquer le conatus avec un exemple concret ?\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ TEST G√âN√âRATION AVEC STATE_IMAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nQuery: {user_input}\")\n",
    "print(f\"\\nSTATE_IMAGE:\")\n",
    "print(json.dumps(test_state_image, indent=2, ensure_ascii=False))\n",
    "\n",
    "# G√©n√©rer\n",
    "result = generator.generate(test_state_image, user_input, system_prompt)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä R√âSULTATS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Contexte: {result['context_tokens']} tokens (objectif: ‚â§500) {'‚úÖ' if result['context_tokens'] <= 500 else '‚ùå'}\")\n",
    "print(f\"Latence: {result['generation_time']:.2f}s\")\n",
    "print(f\"Vitesse: {result['tokens_per_second']:.1f} tokens/s\")\n",
    "print(f\"\\nR√©ponse g√©n√©r√©e:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['response'])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benchmarks Multi-Sc√©narios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir 5 sc√©narios de test\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Question simple\",\n",
    "        \"user_input\": \"C'est quoi le conatus ?\",\n",
    "        \"state_image\": {\n",
    "            \"concepts_actifs\": [\"conatus\"],\n",
    "            \"concepts_rag\": [\"conatus = effort pers√©v√©rer\"],\n",
    "            \"intention\": \"question\",\n",
    "            \"style\": \"concis\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Clarification complexe\",\n",
    "        \"user_input\": \"Je comprends pas le rapport entre conatus et affects\",\n",
    "        \"state_image\": {\n",
    "            \"concepts_actifs\": [\"conatus\", \"affects\", \"rapport\"],\n",
    "            \"concepts_rag\": [\"affects modifient puissance\", \"joie augmente conatus\"],\n",
    "            \"intention\": \"clarification\",\n",
    "            \"tension\": \"confusion\",\n",
    "            \"style\": \"p√©dagogique\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"D√©saccord dialectique\",\n",
    "        \"user_input\": \"Mais on a quand m√™me un libre arbitre non ?\",\n",
    "        \"state_image\": {\n",
    "            \"concepts_actifs\": [\"libre arbitre\", \"libert√©\", \"n√©cessit√©\"],\n",
    "            \"concepts_rag\": [\"libert√© = connaissance n√©cessit√©\", \"illusion libre arbitre\"],\n",
    "            \"intention\": \"d√©saccord\",\n",
    "            \"tension\": \"opposition\",\n",
    "            \"style\": \"p√©dagogique\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Accord progression\",\n",
    "        \"user_input\": \"Ok je vois, donc nos √©motions changent notre puissance d'agir ?\",\n",
    "        \"state_image\": {\n",
    "            \"concepts_actifs\": [\"√©motions\", \"affects\", \"puissance d'agir\"],\n",
    "            \"concepts_rag\": [\"joie augmente puissance\", \"tristesse diminue\"],\n",
    "            \"intention\": \"accord\",\n",
    "            \"style\": \"standard\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Exemple concret demand√©\",\n",
    "        \"user_input\": \"Tu peux donner un exemple avec les r√©seaux sociaux ?\",\n",
    "        \"state_image\": {\n",
    "            \"concepts_actifs\": [\"exemple\", \"concret\", \"r√©seaux sociaux\"],\n",
    "            \"concepts_rag\": [\"affects quotidiens\", \"conatus moderne\"],\n",
    "            \"intention\": \"clarification\",\n",
    "            \"style\": \"p√©dagogique\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ex√©cuter benchmarks\n",
    "benchmark_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî¨ BENCHMARKS MULTI-SC√âNARIOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\n[{i}/5] {scenario['name']}\")\n",
    "    print(f\"  Query: {scenario['user_input']}\")\n",
    "    \n",
    "    result = generator.generate(\n",
    "        scenario['state_image'],\n",
    "        scenario['user_input'],\n",
    "        system_prompt,\n",
    "        max_new_tokens=200\n",
    "    )\n",
    "    \n",
    "    benchmark_results.append({\n",
    "        \"scenario\": scenario['name'],\n",
    "        **result\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úÖ Contexte: {result['context_tokens']} tokens | Latence: {result['generation_time']:.2f}s\")\n",
    "\n",
    "# Afficher r√©sum√©\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä R√âSUM√â BENCHMARKS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "avg_context_tokens = sum(r['context_tokens'] for r in benchmark_results) / len(benchmark_results)\n",
    "avg_latency = sum(r['generation_time'] for r in benchmark_results) / len(benchmark_results)\n",
    "max_context_tokens = max(r['context_tokens'] for r in benchmark_results)\n",
    "\n",
    "print(f\"Contexte moyen: {avg_context_tokens:.0f} tokens (objectif: ‚â§500)\")\n",
    "print(f\"Contexte max: {max_context_tokens} tokens {'‚úÖ' if max_context_tokens <= 500 else '‚ùå'}\")\n",
    "print(f\"Latence moyenne: {avg_latency:.2f}s\")\n",
    "\n",
    "if max_context_tokens <= 500:\n",
    "    print(\"\\n‚úÖ Tous les sc√©narios respectent la contrainte ‚â§500 tokens !\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Certains sc√©narios d√©passent 500 tokens - √Ä optimiser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder r√©sultats benchmarks\n",
    "benchmark_summary = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"quantization\": \"4-bit\",\n",
    "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "    \"vram_used_gb\": vram_used,\n",
    "    \"metrics\": {\n",
    "        \"avg_context_tokens\": avg_context_tokens,\n",
    "        \"max_context_tokens\": max_context_tokens,\n",
    "        \"avg_latency_s\": avg_latency,\n",
    "        \"context_constraint_respected\": max_context_tokens <= 500\n",
    "    },\n",
    "    \"scenarios\": benchmark_results\n",
    "}\n",
    "\n",
    "with open('/content/mistral_benchmark_results.json', 'w') as f:\n",
    "    json.dump(benchmark_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nüíæ R√©sultats sauvegard√©s: /content/mistral_benchmark_results.json\")\n",
    "\n",
    "# T√©l√©charger\n",
    "from google.colab import files\n",
    "files.download('/content/mistral_benchmark_results.json')\n",
    "\n",
    "print(\"\\n‚úÖ Notebook 3 termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù R√©sum√©\n",
    "\n",
    "### ‚úÖ Impl√©ment√©\n",
    "- ‚úÖ Mistral 7B charg√© (4-bit quantization pour T4)\n",
    "- ‚úÖ G√©n√©ration avec STATE_IMAGE structur√©\n",
    "- ‚úÖ Validation contrainte ‚â§500 tokens\n",
    "- ‚úÖ Benchmarks 5 sc√©narios (question, clarification, d√©saccord, etc.)\n",
    "- ‚úÖ Mesure latence et vitesse g√©n√©ration\n",
    "\n",
    "### üìä M√©triques Cibles POC\n",
    "- **Contexte moyen:** ~XXX tokens (objectif: ‚â§500) ‚úÖ\n",
    "- **Contexte max:** XXX tokens (critique: ‚â§500) ‚úÖ\n",
    "- **Latence moyenne:** ~X.Xs (objectif: <3s) ‚úÖ\n",
    "- **VRAM utilis√©e:** ~X.X GB (T4 15GB) ‚úÖ\n",
    "\n",
    "### üéØ Validation POC\n",
    "- ‚úÖ Mistral 7B fonctionnel sur Colab gratuit\n",
    "- ‚úÖ STATE_IMAGE correctement int√©gr√© dans prompt\n",
    "- ‚úÖ Contrainte 500 tokens respect√©e\n",
    "- ‚úÖ Latence acceptable (<3s)\n",
    "\n",
    "### ‚û°Ô∏è Prochaines √âtapes\n",
    "1. **Int√©gration compl√®te:** BERT + RAG + Mistral (Notebook 4 ou Vast.ai)\n",
    "2. **Benchmarks comparatifs:** TRM vs Qwen 14B (n√©cessite Vast.ai)\n",
    "3. **Optimisation:** R√©duire latence, affiner STATE_IMAGE\n",
    "\n",
    "---\n",
    "\n",
    "**üí∞ Co√ªt:** 0‚Ç¨ (Colab gratuit GPU T4 - sessions 12h)\n",
    "\n",
    "**‚è±Ô∏è Temps:** ~2-3h (chargement + tests)\n",
    "\n",
    "**üéØ Objectif Phase 0:** Mistral 7B valid√© pour TRM ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limites Colab Gratuit\n",
    "\n",
    "- **Sessions 12h max** ‚Üí Sauvegarder r√©sultats r√©guli√®rement\n",
    "- **Pas de persistance** ‚Üí T√©l√©charger outputs importants\n",
    "- **T4 15GB VRAM** ‚Üí Quantization 4-bit obligatoire\n",
    "- **Pas d'int√©gration BERT+Mistral simultan√©e** ‚Üí N√©cessite Vast.ai (Phase 1)\n",
    "\n",
    "Pour pipeline complet TRM, passer √† **Phase 1 (Vast.ai/RunPod avec 100‚Ç¨)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
