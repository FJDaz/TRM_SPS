{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ TRM POC - Dialogue avec Spinoza (Mod√®le SPS Fine-tun√©)\n",
    "\n",
    "**Mod√®le:** `FJDaz/spinoza-mistral-7b-merged` (votre mod√®le fine-tun√© !)\n",
    "\n",
    "**Runtime:** GPU Colab gratuit (T4 - 15GB VRAM)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Diff√©rence avec Mistral de base\n",
    "\n",
    "‚úÖ **Mod√®le SPS** (FJDaz/spinoza-mistral-7b-merged) :\n",
    "- Fine-tun√© sp√©cifiquement sur corpus Spinoza\n",
    "- Conna√Æt d√©j√† conatus, affects, √âthique\n",
    "- Meilleure qualit√© dialogue philosophique\n",
    "\n",
    "‚ùå Mistral 7B Instruct (base) :\n",
    "- G√©n√©raliste, pas sp√©cialis√© philo\n",
    "- N√©cessite plus de contexte RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch sentencepiece spacy accelerate bitsandbytes\n",
    "!pip install -q sentence-transformers faiss-gpu gradio\n",
    "!python -m spacy download fr_core_news_sm\n",
    "\n",
    "print(\"‚úÖ D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choix Source Mod√®le SPS\n",
    "\n",
    "**2 options pour charger votre mod√®le SPS :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION MOD√àLE SPS\n",
    "# ============================================\n",
    "\n",
    "# Choisir UNE des 2 options ci-dessous :\n",
    "\n",
    "# --- OPTION A : HuggingFace (Plus simple, mais plus lent 1√®re fois) ---\n",
    "USE_HUGGINGFACE = True\n",
    "MODEL_NAME = \"FJDaz/spinoza-mistral-7b-merged\"\n",
    "\n",
    "# --- OPTION B : Google Drive (Plus rapide si d√©j√† t√©l√©charg√©) ---\n",
    "USE_GOOGLE_DRIVE = False\n",
    "DRIVE_MODEL_PATH = \"/content/drive/MyDrive/spinoza-mistral-7b-merged\"\n",
    "\n",
    "# ============================================\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    print(\"üìÇ Option B : Chargement depuis Google Drive\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    MODEL_PATH = DRIVE_MODEL_PATH\n",
    "    print(f\"‚úÖ Drive mont√© - Mod√®le dans: {MODEL_PATH}\")\n",
    "    \n",
    "elif USE_HUGGINGFACE:\n",
    "    print(\"ü§ó Option A : Chargement depuis HuggingFace\")\n",
    "    MODEL_PATH = MODEL_NAME\n",
    "    print(f\"‚úÖ Mod√®le HF: {MODEL_PATH}\")\n",
    "    print(\"‚è≥ Note: T√©l√©chargement ~14GB la 1√®re fois (5-10 min)\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Activer USE_HUGGINGFACE ou USE_GOOGLE_DRIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload RAG Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload rag_exports.zip (depuis Notebook 2)\\n\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'rag_exports.zip' in uploaded:\n",
    "    !unzip -q rag_exports.zip\n",
    "    RAG_DIR = \"/content/content/rag_exports\"\n",
    "    print(\"‚úÖ RAG exports charg√©s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de RAG - Fonctionnera en mode d√©grad√©\")\n",
    "    RAG_DIR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classes Pipeline (Code r√©utilis√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BERT ENCODER\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import spacy\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "class BERTEncoder:\n",
    "    def __init__(self, model_name: str = \"camembert-base\"):\n",
    "        print(f\"‚è≥ Chargement BERT {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        print(\"‚úÖ BERT charg√© (CPU)\")\n",
    "    \n",
    "    def extract_keywords(self, text: str, top_k: int = 5) -> List[str]:\n",
    "        doc = nlp(text)\n",
    "        entities = [ent.text.lower() for ent in doc.ents]\n",
    "        nouns = [token.text.lower() for token in doc \n",
    "                 if token.pos_ in [\"NOUN\", \"PROPN\"] and len(token.text) > 3]\n",
    "        all_keywords = entities + nouns\n",
    "        counter = Counter(all_keywords)\n",
    "        return [word for word, count in counter.most_common(top_k)]\n",
    "    \n",
    "    def extract_concepts_from_rag(self, rag_passages: List[Dict]) -> List[str]:\n",
    "        concepts = []\n",
    "        for passage in rag_passages:\n",
    "            if passage.get(\"concepts\"):\n",
    "                concepts.extend(passage[\"concepts\"][:3])\n",
    "            else:\n",
    "                text = passage.get(\"text\", \"\")\n",
    "                keywords = self.extract_keywords(text, top_k=3)\n",
    "                concepts.extend(keywords)\n",
    "        return list(dict.fromkeys(concepts))[:8]\n",
    "    \n",
    "    def analyze_intention(self, text: str) -> str:\n",
    "        text_lower = text.lower()\n",
    "        if any(m in text_lower for m in [\"?\", \"comment\", \"pourquoi\"]):\n",
    "            return \"question\"\n",
    "        elif any(m in text_lower for m in [\"d'accord\", \"ok\", \"oui\"]):\n",
    "            return \"accord\"\n",
    "        elif any(m in text_lower for m in [\"non\", \"mais\", \"faux\"]):\n",
    "            return \"d√©saccord\"\n",
    "        return \"neutre\"\n",
    "    \n",
    "    def encode_to_state_image(\n",
    "        self,\n",
    "        conversation: List[Dict],\n",
    "        rag_passages: List[Dict],\n",
    "        prev_state: Optional[Dict] = None,\n",
    "        mini_store_feedback: Optional[Dict] = None\n",
    "    ) -> Dict:\n",
    "        last_exchange = conversation[-1] if conversation else {}\n",
    "        user_text = last_exchange.get(\"user\", \"\")\n",
    "        \n",
    "        return {\n",
    "            \"concepts_actifs\": self.extract_keywords(user_text, top_k=5),\n",
    "            \"concepts_rag\": self.extract_concepts_from_rag(rag_passages),\n",
    "            \"sources_rag\": [p.get(\"source\", \"?\") for p in rag_passages],\n",
    "            \"intention\": self.analyze_intention(user_text),\n",
    "            \"style\": \"p√©dagogique\",\n",
    "            \"metadata\": {\n",
    "                \"philosopher\": \"spinoza\",\n",
    "                \"turn\": (prev_state.get(\"metadata\", {}).get(\"turn\", 0) + 1) if prev_state else 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ BERTEncoder d√©fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RAG RETRIEVER\n",
    "# ============================================================\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, rag_dir: str, philosopher: str = \"spinoza\"):\n",
    "        print(f\"‚è≥ Chargement RAG pour {philosopher}...\")\n",
    "        self.embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.philosopher = philosopher\n",
    "        \n",
    "        index_path = f\"{rag_dir}/{philosopher}_faiss.index\"\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        passages_path = f\"{rag_dir}/{philosopher}_passages.pkl\"\n",
    "        with open(passages_path, 'rb') as f:\n",
    "            self.passages = pickle.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ RAG charg√©: {len(self.passages)} passages\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        query_emb = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        \n",
    "        scores, indices = self.index.search(query_emb, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if score >= 0.45:\n",
    "                passage = self.passages[idx].copy()\n",
    "                passage[\"similarity_score\"] = float(score)\n",
    "                results.append(passage)\n",
    "        \n",
    "        return results[:top_k]\n",
    "\n",
    "print(\"‚úÖ RAGRetriever d√©fini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement Mod√®le SPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SPINOZA MISTRAL GENERATOR (Mod√®le SPS Fine-tun√©)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import time\n",
    "\n",
    "class SpinozaMistralGenerator:\n",
    "    \"\"\"G√©n√©rateur avec mod√®le SPS fine-tun√©\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        print(f\"‚è≥ Chargement SPS (Spinoza) depuis: {model_path}\")\n",
    "        print(\"‚ö†Ô∏è Premi√®re fois: t√©l√©chargement ~14GB (5-10 min)\")\n",
    "        print(\"‚ö†Ô∏è Chargement + quantization: 5-10 min suppl√©mentaires\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Configuration quantization 4-bit (pour T4 15GB)\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        # Charger mod√®le SPS\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            token=None  # Pas de token n√©cessaire pour mod√®le public\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚úÖ SPS charg√© en {load_time:.1f}s\")\n",
    "        \n",
    "        # V√©rifier VRAM\n",
    "        vram_used = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) / 1024**3\n",
    "        print(f\"üìä VRAM utilis√©e: {vram_used:.2f} GB\")\n",
    "    \n",
    "    def format_state_image(self, state_image: Dict) -> str:\n",
    "        lines = []\n",
    "        if state_image.get(\"concepts_actifs\"):\n",
    "            lines.append(f\"Concepts: {', '.join(state_image['concepts_actifs'][:5])}\")\n",
    "        if state_image.get(\"concepts_rag\"):\n",
    "            lines.append(f\"Corpus: {', '.join(state_image['concepts_rag'][:5])}\")\n",
    "        if state_image.get(\"intention\"):\n",
    "            lines.append(f\"Intention: {state_image['intention']}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        state_image: Dict,\n",
    "        user_input: str,\n",
    "        system_prompt: str,\n",
    "        max_new_tokens: int = 300\n",
    "    ) -> str:\n",
    "        state_text = self.format_state_image(state_image)\n",
    "        \n",
    "        prompt = f\"\"\"<s>[INST] {system_prompt}\n",
    "\n",
    "[CONTEXT]\n",
    "{state_text}\n",
    "\n",
    "[QUESTION]\n",
    "{user_input}\n",
    "\n",
    "R√©ponds en incarnant Spinoza. [/INST]\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Charger le mod√®le SPS\n",
    "sps_generator = SpinozaMistralGenerator(MODEL_PATH)\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le SPS pr√™t !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Complet TRM + SPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger composants\n",
    "USE_BERT = True  # Mettre False si VRAM insuffisante\n",
    "\n",
    "print(\"\\nüöÄ Initialisation Pipeline TRM + SPS\")\n",
    "\n",
    "# RAG\n",
    "if RAG_DIR and os.path.exists(f\"{RAG_DIR}/spinoza_faiss.index\"):\n",
    "    rag = RAGRetriever(RAG_DIR, \"spinoza\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è RAG non disponible\")\n",
    "    rag = None\n",
    "\n",
    "# BERT (optionnel)\n",
    "if USE_BERT:\n",
    "    bert = BERTEncoder()\n",
    "else:\n",
    "    bert = None\n",
    "\n",
    "# Prompt syst√®me Spinoza\n",
    "SYSTEM_PROMPT = \"\"\"Tu ES Spinoza. Dialogue p√©dagogique avec √©l√®ve Terminale.\n",
    "Vocabulaire: conatus, affects, puissance d'agir, n√©cessit√© causale.\n",
    "M√©thode: D√©ductions logiques, exemples concrets modernes.\n",
    "R√©ponds en 2-4 phrases max.\"\"\"\n",
    "\n",
    "# Pipeline TRM\n",
    "class TRMPipeline:\n",
    "    def __init__(self, rag, bert, sps, system_prompt):\n",
    "        self.rag = rag\n",
    "        self.bert = bert\n",
    "        self.sps = sps\n",
    "        self.system_prompt = system_prompt\n",
    "        self.conversation_history = []\n",
    "        self.state_image = None\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        # 1. RAG\n",
    "        rag_passages = self.rag.retrieve(user_input, top_k=3) if self.rag else []\n",
    "        \n",
    "        # 2. BERT STATE_IMAGE\n",
    "        if self.bert:\n",
    "            self.conversation_history.append({\"user\": user_input, \"assistant\": \"\"})\n",
    "            self.state_image = self.bert.encode_to_state_image(\n",
    "                self.conversation_history,\n",
    "                rag_passages,\n",
    "                self.state_image,\n",
    "                {}\n",
    "            )\n",
    "        else:\n",
    "            self.state_image = {\n",
    "                \"concepts_actifs\": [],\n",
    "                \"concepts_rag\": [c for p in rag_passages for c in p.get(\"concepts\", [])[:2]],\n",
    "                \"intention\": \"question\"\n",
    "            }\n",
    "        \n",
    "        # 3. SPS Generate\n",
    "        response = self.sps.generate(\n",
    "            self.state_image,\n",
    "            user_input,\n",
    "            self.system_prompt\n",
    "        )\n",
    "        \n",
    "        if self.conversation_history:\n",
    "            self.conversation_history[-1][\"assistant\"] = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def reset(self):\n",
    "        self.conversation_history = []\n",
    "        self.state_image = None\n",
    "\n",
    "# Initialiser pipeline\n",
    "pipeline = TRMPipeline(rag, bert, sps_generator, SYSTEM_PROMPT)\n",
    "\n",
    "print(\"‚úÖ Pipeline TRM + SPS pr√™t !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"C'est quoi le conatus ?\"\n",
    "print(f\"üë§ Vous: {query}\\n\")\n",
    "\n",
    "response = pipeline.chat(query)\n",
    "print(f\"üßô Spinoza (SPS): {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interface Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_interface(user_input, history):\n",
    "    if not user_input.strip():\n",
    "        return history, history\n",
    "    \n",
    "    response = pipeline.chat(user_input)\n",
    "    history.append((user_input, response))\n",
    "    \n",
    "    return history, history\n",
    "\n",
    "def reset_conversation():\n",
    "    pipeline.reset()\n",
    "    return [], []\n",
    "\n",
    "with gr.Blocks(title=\"Spinoza TRM (SPS Fine-tun√©)\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üßô Dialogue avec Spinoza\n",
    "        \n",
    "        **Mod√®le:** FJDaz/spinoza-mistral-7b-merged (fine-tun√©)\n",
    "        \n",
    "        **Architecture TRM:** RAG + BERT + SPS\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400)\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Votre question\",\n",
    "            placeholder=\"Ex: C'est quoi le conatus ?\",\n",
    "            scale=4\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Envoyer\", scale=1)\n",
    "    \n",
    "    clear_btn = gr.Button(\"R√©initialiser\")\n",
    "    \n",
    "    history_state = gr.State([])\n",
    "    \n",
    "    submit_btn.click(\n",
    "        chat_interface,\n",
    "        inputs=[user_input, history_state],\n",
    "        outputs=[chatbot, history_state]\n",
    "    )\n",
    "    \n",
    "    user_input.submit(\n",
    "        chat_interface,\n",
    "        inputs=[user_input, history_state],\n",
    "        outputs=[chatbot, history_state]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        reset_conversation,\n",
    "        outputs=[chatbot, history_state]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\\nüöÄ Interface Gradio lanc√©e avec mod√®le SPS !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "### Mod√®le Utilis√©\n",
    "- **SPS:** `FJDaz/spinoza-mistral-7b-merged` (fine-tun√© Spinoza)\n",
    "- **Avantage:** Meilleure qualit√© dialogue philo vs Mistral base\n",
    "\n",
    "### Options Chargement\n",
    "- **Option A (HF):** Simple, t√©l√©chargement auto ~14GB\n",
    "- **Option B (Drive):** Plus rapide si mod√®le d√©j√† t√©l√©charg√©\n",
    "\n",
    "### Pipeline Complet\n",
    "- ‚úÖ RAG retrieve passages √âthique\n",
    "- ‚úÖ BERT g√©n√®re STATE_IMAGE\n",
    "- ‚úÖ SPS g√©n√®re r√©ponse fine-tun√©e\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Vous dialoguez maintenant avec votre mod√®le SPS fine-tun√© !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
